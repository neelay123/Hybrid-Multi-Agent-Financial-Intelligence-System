{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d264fb-1ce5-4767-beb2-7f8c2d92eab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - nvidia/label/cuda-12.1.1\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 23.11.0\n",
      "    latest version: 25.7.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - nvidia/label/cuda-12.1.1::cuda-nvcc\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2025.8.3   |       hbd8a1cb_0         151 KB  conda-forge\n",
      "    certifi-2025.8.3           |     pyhd8ed1ab_0         155 KB  conda-forge\n",
      "    cuda-nvcc-12.1.105         |                0        52.4 MB  nvidia/label/cuda-12.1.1\n",
      "    libgcc-15.1.0              |       h767d61c_4         805 KB  conda-forge\n",
      "    libgcc-ng-15.1.0           |       h69a702a_4          29 KB  conda-forge\n",
      "    libgomp-15.1.0             |       h767d61c_4         437 KB  conda-forge\n",
      "    openssl-3.5.2              |       h26f9b46_0         3.0 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        56.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cuda-nvcc          nvidia/label/cuda-12.1.1/linux-64::cuda-nvcc-12.1.105-0 \n",
      "  libgcc             conda-forge/linux-64::libgcc-15.1.0-h767d61c_4 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge/linux-64::ca-certificates~ --> conda-forge/noarch::ca-certificates-2025.8.3-hbd8a1cb_0 \n",
      "  certifi                           2023.11.17-pyhd8ed1ab_0 --> 2025.8.3-pyhd8ed1ab_0 \n",
      "  libgcc-ng                               13.2.0-h807b86a_3 --> 15.1.0-h69a702a_4 \n",
      "  libgomp                                 13.2.0-h807b86a_3 --> 15.1.0-h767d61c_4 \n",
      "  openssl                                  3.2.0-hd590300_1 --> 3.5.2-h26f9b46_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "cuda-nvcc-12.1.105   | 52.4 MB   |                                       |   0% \n",
      "openssl-3.5.2        | 3.0 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "libgcc-15.1.0        | 805 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libgomp-15.1.0       | 437 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.8.3     | 155 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 151 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-ng-15.1.0     | 29 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libgcc-15.1.0        | 805 KB    | 7                                     |   2% \u001b[A\u001b[A\n",
      "openssl-3.5.2        | 3.0 MB    | 1                                     |   1% \u001b[A\n",
      "\n",
      "\n",
      "libgomp-15.1.0       | 437 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.8.3     | 155 KB    | ###8                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 151 KB    | ###9                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-ng-15.1.0     | 29 KB     | ####################7                 |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.5.2        | 3.0 MB    | ##################################8   |  94% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 151 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libgomp-15.1.0       | 437 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libgomp-15.1.0       | 437 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.8.3     | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.8.3     | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libgcc-15.1.0        | 805 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "libgcc-15.1.0        | 805 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-ng-15.1.0     | 29 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-ng-15.1.0     | 29 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting trl\n",
      "  Using cached trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting kaggle\n",
      "  Using cached kaggle-1.7.4.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (23.2)\n",
      "Collecting ninja\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (5.9.8)\n",
      "Collecting torch>=1.13.0 (from peft)\n",
      "  Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.11/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in /opt/conda/lib/python3.11/site-packages (from kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from kaggle) (3.6)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from kaggle) (4.25.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.8.2)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from kaggle) (69.0.3)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.1.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from kaggle) (0.5.1)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras)\n",
      "  Using cached tensorflow-2.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.9.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.64.1)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached keras-3.11.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.11.0)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.37.1)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.13.0->peft)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.13.0->peft)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.13.0->peft)\n",
      "  Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.42.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Using cached transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "Using cached trl-0.21.0-py3-none-any.whl (511 kB)\n",
      "Using cached accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Using cached kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
      "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached tensorflow-2.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached keras-3.11.2-py3-none-any.whl (1.4 MB)\n",
      "Using cached ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: text-unidecode, nvidia-cusparselt-cu12, xxhash, typing-extensions, triton, tqdm, sympy, safetensors, requests, python-slugify, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, ml-dtypes, hf-xet, dill, tensorboard, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, kaggle, huggingface-hub, tokenizers, nvidia-cusolver-cu12, keras, transformers, torch, tensorflow, datasets, tf-keras, accelerate, trl, peft\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.3.1\n",
      "    Uninstalling triton-2.3.1:\n",
      "      Successfully uninstalled triton-2.3.1\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12.1\n",
      "    Uninstalling sympy-1.12.1:\n",
      "      Successfully uninstalled sympy-1.12.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.2\n",
      "    Uninstalling ml-dtypes-0.3.2:\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.16.2\n",
      "    Uninstalling tensorflow-2.16.2:\n",
      "      Successfully uninstalled tensorflow-2.16.2\n",
      "Successfully installed accelerate-1.10.0 datasets-4.0.0 dill-0.3.8 hf-xet-1.1.8 huggingface-hub-0.34.4 kaggle-1.7.4.5 keras-3.11.2 ml-dtypes-0.5.3 multiprocess-0.70.16 ninja-1.13.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 peft-0.17.0 pyarrow-21.0.0 python-slugify-8.0.4 requests-2.32.5 safetensors-0.6.2 sympy-1.14.0 tensorboard-2.19.0 tensorflow-2.19.1 text-unidecode-1.3 tf-keras-2.19.0 tokenizers-0.21.4 torch-2.8.0 tqdm-4.67.1 transformers-4.55.2 triton-3.4.0 trl-0.21.0 typing-extensions-4.14.1 xxhash-3.5.0\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.8.0)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/conda/lib/python3.11/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.4.0->torch) (69.0.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.3.90->torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n",
      "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
      "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0\n",
      "    Uninstalling torch-2.8.0:\n",
      "      Successfully uninstalled torch-2.8.0\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.32.5)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (3.3)\n",
      "Collecting alpha_vantage\n",
      "  Using cached alpha_vantage-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting redis\n",
      "  Using cached redis-6.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sec_api\n",
      "  Using cached sec_api-1.0.32-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting cachetools\n",
      "  Using cached cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from alpha_vantage) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (1.20.1)\n",
      "Using cached alpha_vantage-3.0.0-py3-none-any.whl (35 kB)\n",
      "Using cached redis-6.4.0-py3-none-any.whl (279 kB)\n",
      "Using cached sec_api-1.0.32-py3-none-any.whl (24 kB)\n",
      "Using cached cachetools-6.1.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: redis, cachetools, sec_api, alpha_vantage\n",
      "Successfully installed alpha_vantage-3.0.0 cachetools-6.1.0 redis-6.4.0 sec_api-1.0.32\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.5.0+cu121.html\n",
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting torch_sparse\n",
      "  Using cached https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_sparse-0.6.18%2Bpt25cu121-cp311-cp311-linux_x86_64.whl (5.1 MB)\n",
      "Collecting torch_scatter\n",
      "  Using cached https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_scatter-2.1.2%2Bpt25cu121-cp311-cp311-linux_x86_64.whl (10.9 MB)\n",
      "Collecting torch_cluster\n",
      "  Using cached https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_cluster-1.6.3%2Bpt25cu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.12.13)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from torch_sparse) (1.14.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2025.8.3)\n",
      "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "Installing collected packages: torch_scatter, torch_sparse, torch_cluster, torch_geometric\n",
      "Successfully installed torch_cluster-1.6.3+pt25cu121 torch_geometric-2.6.1 torch_scatter-2.1.2+pt25cu121 torch_sparse-0.6.18+pt25cu121\n",
      "Collecting torch-geometric-temporal==0.54.0\n",
      "  Using cached torch_geometric_temporal-0.54.0-py3-none-any.whl\n",
      "Collecting decorator==4.4.2 (from torch-geometric-temporal==0.54.0)\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from torch-geometric-temporal==0.54.0) (2.5.1+cu121)\n",
      "Collecting cython (from torch-geometric-temporal==0.54.0)\n",
      "  Using cached cython-3.1.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting pandas<=1.3.5 (from torch-geometric-temporal==0.54.0)\n",
      "  Using cached pandas-1.3.5-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: torch-sparse in /opt/conda/lib/python3.11/site-packages (from torch-geometric-temporal==0.54.0) (0.6.18+pt25cu121)\n",
      "Requirement already satisfied: torch-scatter in /opt/conda/lib/python3.11/site-packages (from torch-geometric-temporal==0.54.0) (2.1.2+pt25cu121)\n",
      "Requirement already satisfied: torch-geometric in /opt/conda/lib/python3.11/site-packages (from torch-geometric-temporal==0.54.0) (2.6.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torch-geometric-temporal==0.54.0) (1.26.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from torch-geometric-temporal==0.54.0) (1.16.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch-geometric-temporal==0.54.0) (3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.11/site-packages (from pandas<=1.3.5->torch-geometric-temporal==0.54.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.11/site-packages (from pandas<=1.3.5->torch-geometric-temporal==0.54.0) (2023.3.post1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (4.14.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->torch-geometric-temporal==0.54.0) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torch-geometric-temporal==0.54.0) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->torch-geometric-temporal==0.54.0) (1.3.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from torch-geometric->torch-geometric-temporal==0.54.0) (3.12.13)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.11/site-packages (from torch-geometric->torch-geometric-temporal==0.54.0) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.11/site-packages (from torch-geometric->torch-geometric-temporal==0.54.0) (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torch-geometric->torch-geometric-temporal==0.54.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from torch-geometric->torch-geometric-temporal==0.54.0) (4.67.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from torch-sparse->torch-geometric-temporal==0.54.0) (1.14.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch-geometric->torch-geometric-temporal==0.54.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch-geometric->torch-geometric-temporal==0.54.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch-geometric->torch-geometric-temporal==0.54.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch-geometric->torch-geometric-temporal==0.54.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch-geometric->torch-geometric-temporal==0.54.0) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch-geometric->torch-geometric-temporal==0.54.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch-geometric->torch-geometric-temporal==0.54.0) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->torch-geometric-temporal==0.54.0) (2.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torch-geometric->torch-geometric-temporal==0.54.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torch-geometric->torch-geometric-temporal==0.54.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->torch-geometric->torch-geometric-temporal==0.54.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torch-geometric->torch-geometric-temporal==0.54.0) (2025.8.3)\n",
      "Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Using cached cython-3.1.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "Installing collected packages: decorator, cython, pandas, torch-geometric-temporal\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "Successfully installed cython-3.1.3 decorator-4.4.2 pandas-1.3.5 torch-geometric-temporal-0.54.0\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting yfinance\n",
      "  Using cached yfinance-0.2.65-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting qdrant-client\n",
      "  Using cached qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from yfinance) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2.32.5)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Using cached multitasking-0.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from yfinance) (4.1.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2023.3.post1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Using cached frozendict-2.4.6-py311-none-any.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Using cached peewee-3.18.2-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/conda/lib/python3.11/site-packages (from yfinance) (4.12.3)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Using cached curl_cffi-0.13.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from yfinance) (4.25.3)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /opt/conda/lib/python3.11/site-packages (from qdrant-client) (1.64.1)\n",
      "Collecting httpx>=0.20.0 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
      "  Using cached portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /opt/conda/lib/python3.11/site-packages (from qdrant-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /opt/conda/lib/python3.11/site-packages (from qdrant-client) (2.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from curl_cffi>=0.7->yfinance) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in /opt/conda/lib/python3.11/site-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Collecting httpcore==1.* (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.6)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.11/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (4.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.21)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.0)\n",
      "Using cached faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "Using cached yfinance-0.2.65-py2.py3-none-any.whl (119 kB)\n",
      "Using cached qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached curl_cffi-0.13.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Using cached frozendict-2.4.6-py311-none-any.whl (16 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Using cached h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: peewee, multitasking, websockets, portalocker, hyperframe, hpack, h11, frozendict, faiss-cpu, httpcore, h2, curl_cffi, yfinance, seaborn, httpx, qdrant-client\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 12.0\n",
      "    Uninstalling websockets-12.0:\n",
      "      Successfully uninstalled websockets-12.0\n",
      "Successfully installed curl_cffi-0.13.0 faiss-cpu-1.12.0 frozendict-2.4.6 h11-0.16.0 h2-4.2.0 hpack-4.1.0 httpcore-1.0.9 httpx-0.28.1 hyperframe-6.1.0 multitasking-0.0.12 peewee-3.18.2 portalocker-3.2.0 qdrant-client-1.15.1 seaborn-0.13.2 websockets-15.0.1 yfinance-0.2.65\n"
     ]
    }
   ],
   "source": [
    "!conda install nvidia/label/cuda-12.1.1::cuda-nvcc -y\n",
    "!pip install transformers datasets peft trl accelerate kaggle packaging ninja tf-keras\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install requests networkx alpha_vantage redis sec_api cachetools\n",
    "!pip install torch_geometric torch_sparse torch_scatter torch_cluster -f https://data.pyg.org/whl/torch-2.5.0+cu121.html\n",
    "!pip install torch-geometric-temporal==0.54.0\n",
    "!pip install scikit-learn matplotlib faiss-cpu yfinance qdrant-client seaborn\n",
    "!pip install graphrag graspologic\n",
    "!pip install langgraph langchain langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86217efd-e0ce-4897-85e6-681a4b2d92ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.8.3-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from flash-attn) (2.5.1+cu121)\n",
      "Collecting einops (from flash-attn)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (4.14.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->flash-attn) (2.1.4)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.1 flash-attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cfdda6-4394-49e3-afb5-add916c24bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755709504.032655    2568 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755709504.060730    2568 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755709504.256357    2568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755709504.256400    2568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755709504.256403    2568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755709504.256405    2568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "INFO:__main__:GPU memory STRICTLY limited to 90.0% of available memory\n",
      "INFO:__main__: Initializing inference-ready Financial AI System...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initializing Financial AI System...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [02:10<00:00, 21.79s/it]\n",
      "INFO:__main__:Fine-tuned Phi-4 model loaded successfully\n",
      "INFO:__main__:Loaded intfloat/multilingual-e5-large on cuda\n",
      "INFO:__main__:Connected to Qdrant local mode at ./financial_embeddings_db\n",
      "INFO:__main__: Loaded a total of 17552 points from collection 'financial_entities_graphsage'\n",
      "INFO:__main__: Loaded 17552 graphsage embeddings\n",
      "INFO:__main__: Loaded a total of 17552 points from collection 'financial_entities_attention_gnn'\n",
      "INFO:__main__: Loaded 17552 attention_gnn embeddings\n",
      "INFO:__main__: Loaded a total of 17552 points from collection 'financial_entities_temporal_gnn'\n",
      "INFO:__main__: Loaded 17552 temporal_gnn embeddings\n",
      "INFO:__main__: Loaded a total of 16333 points from collection 'financial_entities_e5'\n",
      "INFO:__main__: Loaded 16333 e5 embeddings\n",
      "INFO:__main__: Loaded 68989 embeddings from 4 model types\n",
      "/tmp/ipykernel_2568/3283767891.py:3073: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  gnn_model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "INFO:__main__: Loaded trained GNN and KG from 'trained_models_20250815_193553'\n",
      "INFO:__main__:Successfully created 11 communities\n",
      "INFO:__main__:Initializing FAISS index for 11 community summaries.\n",
      "INFO:__main__:Community index created successfully.\n",
      "INFO:__main__: System initialized successfully.\n",
      "INFO:__main__: System Status: {'available_embedding_models': ['graphsage', 'attention_gnn', 'temporal_gnn', 'e5'], 'total_entities_loaded': 17552, 'gnn_model_loaded': True, 'graphrag_engine_ready': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " System is ready. Enter your financial query or type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">   Show me Microsoft's stock price?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'microsoft corporation'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Microsoft Corporation'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'microsoft corporation' to 'MICROSOFT CORP'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n",
      "INFO:__main__: ticker MSFT found for MICROSOFT CORP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Successfully fetched Yahoo Finance data for MICROSOFT CORP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Microsoft Corporation's price is 505.25. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What is Delta's dividend yield?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'delta airlines inc'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Delta Airlines Inc'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'delta airlines inc' to 'DELTA AIR LINES, INC.'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n",
      "INFO:__main__: ticker DAL found for DELTA AIR LINES, INC.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Successfully fetched Yahoo Finance data for DELTA AIR LINES, INC.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Delta Air Lines, Inc.'s dividend yield is 1.24. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">   What is Amazon's revenue?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'amazon.com inc'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Amazon.Com Inc'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'amazon.com inc' to 'AMAZON COM INC'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n",
      "INFO:__main__: ticker AMZN.MX found for AMAZON COM INC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Successfully fetched Yahoo Finance data for AMAZON COM INC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Amazon.com, Inc.'s revenue is $670.04B. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Show me Netflix's beta coefficient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'netflix inc'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Netflix Inc'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'netflix inc' to 'NETFLIX INC'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n",
      "INFO:__main__: ticker NFLX found for NETFLIX INC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Successfully fetched Yahoo Finance data for NETFLIX INC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Netflix, Inc.'s beta is 1.59. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What is JPMorgan Chase's total debt?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'jpmorgan chase & co'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Jpmorgan Chase & Co'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'jpmorgan chase & co' to 'JPMORGAN CHASE & CO'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n",
      "INFO:__main__: ticker JPM found for JPMORGAN CHASE & CO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Successfully fetched Yahoo Finance data for JPMORGAN CHASE & CO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "JPMorgan Chase & Co.'s total debt is $1165.54B. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What is Nvidia's return on equity?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'nvidia corporation'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Nvidia Corporation'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'nvidia corporation' to 'NVIDIA CORP'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n",
      "INFO:__main__: ticker NVDA found for NVIDIA CORP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Successfully fetched Yahoo Finance data for NVIDIA CORP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "NVIDIA Corporation's roe is 1.15. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What is Tesla's current stock price?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'What is Tesla current stock price'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'What Is Tesla Current Stock Price'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'What is Tesla current stock price' to 'Tesla, Inc.'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: ticker TSLA found for Tesla, Inc.\n",
      "INFO:__main__: Successfully fetched Yahoo Finance data for Tesla, Inc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Tesla, Inc.'s price is 321.83. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What is AMD's return on equity?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'amd'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Amd'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'amd' to 'ADVANCED MICRO DEVICES INC'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n",
      "INFO:__main__: ticker AMD found for ADVANCED MICRO DEVICES INC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Successfully fetched Yahoo Finance data for ADVANCED MICRO DEVICES INC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Advanced Micro Devices, Inc.'s roe is 0.05. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What is Delta's price to book ratio?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'delta airlines inc'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Delta Airlines Inc'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'delta airlines inc' to 'DELTA AIR LINES, INC.'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n",
      "INFO:__main__: ticker DAL found for DELTA AIR LINES, INC.\n",
      "INFO:__main__: Successfully fetched Yahoo Finance data for DELTA AIR LINES, INC.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Delta Air Lines, Inc.'s price is 59.25. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What is the P/E ratio of Apple?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'apple inc'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Apple Inc'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'apple inc' to 'Apple Inc.'.\n",
      "INFO:__main__:Decision: Routing to global path (RAG).\n",
      "INFO:__main__: ticker AAPL found for Apple Inc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Successfully fetched Yahoo Finance data for Apple Inc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Apple Inc.'s pe ratio is 34.41. (Source: Live data from Yahoo Finance)\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Find companies similar to Apple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'Find companies similar to Apple'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Find Companies Similar To Apple'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'Find companies similar to Apple' to 'Apple Inc.'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: Apple Inc. (ID: 320193)\n",
      "INFO:__main__: Subgraph size: 161 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for Apple Inc.\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'Apple Inc. Electronic Computers'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for Apple Inc.:\n",
      "\n",
      "1. DELL INC\n",
      "2. Silicon Graphics International Corp\n",
      "3. EVANS & SUTHERLAND COMPUTER CORP\n",
      "4. ELECTRONICS FOR IMAGING INC\n",
      "5. ECHELON CORP\n",
      "6. DigitalTown, Inc.\n",
      "7. Sector 5, Inc.\n",
      "8. EMC CORP\n",
      "9. QLOGIC CORP\n",
      "10. CRAY INC\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Who are Tesla's main competitors?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'tesla inc'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Tesla Inc'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'tesla inc' to 'Tesla, Inc.'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: Tesla, Inc. (ID: 1318605)\n",
      "INFO:__main__: Subgraph size: 162 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for Tesla, Inc.\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'Tesla, Inc. Motor Vehicles & Passenger Car Bodies'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for Tesla, Inc.:\n",
      "\n",
      "1. Arrival\n",
      "2. ELECTRAMECCANICA VEHICLES CORP.\n",
      "3. Lightning eMotors, Inc.\n",
      "4. IDEANOMICS, INC.\n",
      "5. TATA MOTORS LTD/FI\n",
      "6. T3M INC.\n",
      "7. TOYOTA MOTOR CORP/\n",
      "8. Leo Motors, Inc.\n",
      "9. FCA US LLC\n",
      "10. Motors Liquidation Co\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Find peers of JPMorgan Chase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'Find peers of JPMorgan Chase'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Find Peers Of Jpmorgan Chase'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'Find peers of JPMorgan Chase' to 'JPMORGAN CHASE & CO'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: JPMORGAN CHASE & CO (ID: 19617)\n",
      "INFO:__main__: Subgraph size: 166 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for JPMORGAN CHASE & CO\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'JPMORGAN CHASE & CO National Commercial Banks'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for JPMORGAN CHASE & CO:\n",
      "\n",
      "1. National Commerce Corp\n",
      "2. COMMERCIAL NATIONAL FINANCIAL CORP /PA\n",
      "3. COMMUNITYCORP\n",
      "4. JEFFERSONVILLE BANCORP\n",
      "5. CITIGROUP INC\n",
      "6. Professional Holding Corp.\n",
      "7. CHEVIOT FINANCIAL CORP\n",
      "8. CITY NATIONAL CORP\n",
      "9. Chino Commercial Bancorp\n",
      "10. PEOPLES BANCORP INC/MD\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Show me companies like goldman sachs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'Show me companies like goldman sachs'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Show Me Companies Like Goldman Sachs'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'Show me companies like goldman sachs' to 'GOLDMAN SACHS GROUP INC'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: GOLDMAN SACHS GROUP INC (ID: 886982)\n",
      "INFO:__main__: Subgraph size: 165 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for GOLDMAN SACHS GROUP INC\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'GOLDMAN SACHS GROUP INC Security Brokers, Dealers & Flotation Companies'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for GOLDMAN SACHS GROUP INC:\n",
      "\n",
      "1. GILMAN CIOCIA, INC.\n",
      "2. SCHWAB CHARLES CORP\n",
      "3. MORGAN STANLEY\n",
      "4. Jefferies Group LLC\n",
      "5. SUMMIT FINANCIAL SERVICES GROUP INC\n",
      "6. GLEACHER & COMPANY, INC.\n",
      "7. Cohen & Co Inc.\n",
      "8. GREENHILL & CO INC\n",
      "9. GAMCO INVESTORS, INC. ET AL\n",
      "10. PENSON WORLDWIDE INC\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Find companies that compete with Walmart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'Find companies that compete with Walmart'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Find Companies That Compete With Walmart'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'Find companies that compete with Walmart' to 'Walmart Inc.'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: Walmart Inc. (ID: 104169)\n",
      "INFO:__main__: Subgraph size: 163 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for Walmart Inc.\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'Walmart Inc. Retail-Variety Stores'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for Walmart Inc.:\n",
      "\n",
      "1. ALCO STORES INC\n",
      "2. TUESDAY MORNING CORP/DE\n",
      "3. BIG LOTS INC\n",
      "4. PRICESMART INC\n",
      "5. TARGET CORP\n",
      "6. FREDS INC\n",
      "7. WALGREEN CO\n",
      "8. STAPLES INC\n",
      "9. FAMILY DOLLAR STORES INC\n",
      "10. WINMARK CORP\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What companies are like Coca-Cola?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'What companies are like Coca-Cola'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'What Companies Are Like Coca-Cola'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'What companies are like Coca-Cola' to 'Coca-Cola Consolidated, Inc.'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: Coca-Cola Consolidated, Inc. (ID: 317540)\n",
      "INFO:__main__: Subgraph size: 164 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for Coca-Cola Consolidated, Inc.\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'Coca-Cola Consolidated, Inc. Bottled & Canned Soft Drinks & Carbonated Waters'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for Coca-Cola Consolidated, Inc.:\n",
      "\n",
      "1. COCA-COLA REFRESHMENTS USA, INC.\n",
      "2. COCA COLA FEMSA SAB DE CV\n",
      "3. COCA-COLA EUROPEAN PARTNERS US, LLC\n",
      "4. COCA-COLA EUROPACIFIC PARTNERS plc\n",
      "5. ANDINA BOTTLING CO INC\n",
      "6. Primo Water Corp /CN/\n",
      "7. Cell-nique Corp\n",
      "8. Celsius Holdings, Inc.\n",
      "9. NATIONAL BEVERAGE CORP\n",
      "10. Blue Line Holdings, Inc.\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What pharmaceutical companies are like Pfizer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'What pharmaceutical companies are like Pfizer'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'What Pharmaceutical Companies Are Like Pfizer'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'What pharmaceutical companies are like Pfizer' to 'PFIZER INC'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: PFIZER INC (ID: 78003)\n",
      "INFO:__main__: Subgraph size: 109 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for PFIZER INC\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'PFIZER INC Pharmaceutical Preparations'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for PFIZER INC:\n",
      "\n",
      "1. POZEN INC /NC\n",
      "2. PHYSICIANS FORMULA HOLDINGS, INC.\n",
      "3. PHARMACYCLICS INC\n",
      "4. Pfenex Inc.\n",
      "5. PSYENCE BIOMEDICAL LTD.\n",
      "6. MEDICINES CO /DE\n",
      "7. ADVANZ PHARMA Corp.\n",
      "8. Pharmasset Inc\n",
      "9. PHASERX, INC.\n",
      "10. MEDICURE INC\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  What companies are similar to toyota?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'What companies are similar to toyota'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'What Companies Are Similar To Toyota'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'What companies are similar to toyota' to 'TOYOTA MOTOR CORP/'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: TOYOTA MOTOR CORP/ (ID: 1094517)\n",
      "INFO:__main__: Subgraph size: 160 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for TOYOTA MOTOR CORP/\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'TOYOTA MOTOR CORP/ Motor Vehicles & Passenger Car Bodies'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for TOYOTA MOTOR CORP/:\n",
      "\n",
      "1. TATA MOTORS LTD/FI\n",
      "2. Arrival\n",
      "3. Motors Liquidation Co\n",
      "4. T3M INC.\n",
      "5. FORD MOTOR CO\n",
      "6. TOYOTA MOTOR CREDIT CORP\n",
      "7. General Motors Co\n",
      "8. Proterra Inc\n",
      "9. IDEANOMICS, INC.\n",
      "10. HONDA MOTOR CO LTD\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Show me banks similar to Wells Fargo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'wells fargo & co'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Wells Fargo & Co'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'wells fargo & co' to 'WELLS FARGO & COMPANY/MN'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: WELLS FARGO & COMPANY/MN (ID: 72971)\n",
      "INFO:__main__: Subgraph size: 164 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for WELLS FARGO & COMPANY/MN\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'WELLS FARGO & COMPANY/MN National Commercial Banks'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for WELLS FARGO & COMPANY/MN:\n",
      "\n",
      "1. MIDDLEBURG FINANCIAL CORP\n",
      "2. NORWOOD FINANCIAL CORP\n",
      "3. CITIZENS & NORTHERN CORP\n",
      "4. MW Bancorp, Inc.\n",
      "5. CULLEN/FROST BANKERS, INC.\n",
      "6. MID WISCONSIN FINANCIAL SERVICES INC\n",
      "7. NORTHERN STATES FINANCIAL CORP /DE/\n",
      "8. FB Corp\n",
      "9. MARSHALL & ILSLEY CORP\n",
      "10. PEOPLES BANCORP INC/MD\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Find semiconductor companies like nvidia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'Find semiconductor companies like nvidia'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Find Semiconductor Companies Like Nvidia'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'Find semiconductor companies like nvidia' to 'NVIDIA CORP'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: NVIDIA CORP (ID: 1045810)\n",
      "INFO:__main__: Subgraph size: 154 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for NVIDIA CORP\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'NVIDIA CORP Semiconductors & Related Devices'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for NVIDIA CORP:\n",
      "\n",
      "1. NEOPHOTONICS CORP\n",
      "2. NATIONAL SEMICONDUCTOR CORP\n",
      "3. SEMICONDUCTOR MANUFACTURING INTERNATIONAL CORP\n",
      "4. LOGIC DEVICES Inc\n",
      "5. ENERGY CONVERSION DEVICES INC\n",
      "6. China Inc\n",
      "7. E DIGITAL CORP\n",
      "8. OPTI INC\n",
      "9. LSI CORP\n",
      "10. NXP Semiconductors N.V.\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Who are main competitors for ford?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'ford motor co'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Ford Motor Co'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'ford motor co' to 'FORD MOTOR CREDIT CO LLC'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: FORD MOTOR CREDIT CO LLC (ID: 38009)\n",
      "INFO:__main__: Subgraph size: 131 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for FORD MOTOR CREDIT CO LLC\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'FORD MOTOR CREDIT CO LLC Miscellaneous Business Credit Institution'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for FORD MOTOR CREDIT CO LLC:\n",
      "\n",
      "1. Franklin Credit Management Corp\n",
      "2. PHH CORP\n",
      "3. TOYOTA MOTOR CREDIT CORP\n",
      "4. MICROFINANCIAL INC\n",
      "5. IBM CREDIT LLC\n",
      "6. CATERPILLAR FINANCIAL SERVICES CORP\n",
      "7. Consumer Capital Group, Inc.\n",
      "8. AMERICAN HONDA FINANCE CORP\n",
      "9. United Development Funding III, LP\n",
      "10. CNH Industrial Capital LLC\n",
      "====================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">  Show me companies similar to Boeing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Entering Node: query_analysis \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing your query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting exact match for 'Show me companies similar to Boeing'...\n",
      "INFO:__main__:Attempting case-insensitive match for 'Show Me Companies Similar To Boeing'...\n",
      "/tmp/ipykernel_2568/3283767891.py:1601: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.client.search(\n",
      "INFO:__main__: Resolved 'Show me companies similar to Boeing' to 'BOEING CO'.\n",
      "INFO:__main__:Decision: Routing to local path (GNN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: query_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "INFO:__main__: Running memory-efficient GNN similarity search for: BOEING CO (ID: 12927)\n",
      "INFO:__main__: Subgraph size: 165 nodes (vs full graph: 17552)\n",
      "INFO:__main__: Found 10 similar companies for BOEING CO\n",
      "INFO:__main__: Semantic competitor query detected. Using keyword-based query for E5 search: 'BOEING CO Aircraft'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Step completed: gnn_inference\n",
      "  -> Step completed: generate_response\n",
      "\n",
      "================================================== FINAL RESPONSE ==================================================\n",
      "Based on semantic analysis, here are the top competitors for BOEING CO:\n",
      "\n",
      "1. BOEING CAPITAL CORP\n",
      "2. B/E AEROSPACE INC\n",
      "3. Stealth Air Corp.\n",
      "4. BOA Acquisition Corp.\n",
      "5. HAWKER BEECHCRAFT ACQUISITION CO LLC\n",
      "6. BRUNSWICK CORP\n",
      "7. Bantec, Inc.\n",
      "8. Copa Holdings, S.A.\n",
      "9. Eviation Aircraft Ltd.\n",
      "10. DUCOMMUN INC /DE/\n",
      "====================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# UNIFIED FINANCIAL AI SYSTEM WITH GRAPH EMBEDDINGS AND AGENTIC REASONING\n",
    "# \n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import time\n",
    "import asyncio\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union, TypedDict, Annotated, Literal\n",
    "from functools import partial\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.nn import HeteroConv, GATv2Conv\n",
    "\n",
    "import uuid\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ML and NLP libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, AutoModel\n",
    "from peft import get_peft_model, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, load_dataset\n",
    "import faiss\n",
    "\n",
    "\n",
    "\n",
    "# Storage and caching\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    VectorParams, Distance, PointStruct, Filter, FieldCondition, \n",
    "    Range, MatchValue, PayloadSchemaType\n",
    ")\n",
    "from cachetools import LRUCache, TTLCache, LFUCache\n",
    "\n",
    "import random; random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Optional SEC API\n",
    "try:\n",
    "    from sec_api import ExtractorApi\n",
    "except ImportError:\n",
    "    ExtractorApi = None\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Replace device_map=\"auto\" with specific device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# \n",
    "# CONFIGURATION AND DATA MODELS\n",
    "# \n",
    "\n",
    "@dataclass\n",
    "class FinancialEntity:\n",
    "    \"\"\"Represents a financial entity in the knowledge graph\"\"\"\n",
    "    name: str\n",
    "    entity_type: str  # company, metric, regulation, person\n",
    "    ticker: Optional[str] = None\n",
    "    sector: Optional[str] = None\n",
    "    attributes: Dict[str, Any] = None\n",
    "\n",
    "@dataclass\n",
    "class FactCheckResult:\n",
    "    \"\"\"Results from fact verification process\"\"\"\n",
    "    claim: str\n",
    "    verified: bool\n",
    "    confidence: float\n",
    "    supporting_evidence: List[str]\n",
    "    contradicting_evidence: List[str]\n",
    "    timestamp: datetime\n",
    "\n",
    "@dataclass\n",
    "class KnowledgeGraphEdge:\n",
    "    \"\"\"Represents relationships in the knowledge graph\"\"\"\n",
    "    source: str\n",
    "    target: str\n",
    "    relationship_type: str\n",
    "    confidence: float\n",
    "    source_document: str\n",
    "    timestamp: datetime\n",
    "\n",
    "class SystemConfig:\n",
    "    \"\"\"Unified configuration settings for the financial AI system\"\"\"\n",
    "    def __init__(self):\n",
    "        # Model configuration\n",
    "        self.phi4_model_path = \"microsoft/phi-4\"\n",
    "        self.fine_tuned_model_path = \"./phi4-finqa-final\"\n",
    "        self.max_sequence_length = 8192  \n",
    "        self.embedding_model = \"intfloat/multilingual-e5-large\"\n",
    "        self.vector_db_dimension = 1024\n",
    "        \n",
    "        # System configuration\n",
    "        self.cache_ttl = 3600  # 1 hour\n",
    "        self.fact_check_threshold = 0.8\n",
    "        self.max_retrieval_docs = 10\n",
    "        \n",
    "        # API keys\n",
    "        self.sec_api_key = os.getenv('sec_api')\n",
    "        self.alpha_vantage_key = 'YWQ6A107QAJ1TPQR'\n",
    "\n",
    "        # Add SEC EDGAR specific config:\n",
    "        self.sec_user_agent = \"Financial Research Bot research@example.com\"\n",
    "        self.sec_rate_limit = 0.11  # 10 requests per second = 0.1s, add buffer\n",
    "        \n",
    "        # Graph configuration\n",
    "        self.embedding_dim = 1024\n",
    "        self.graph_storage_path = \"./financial_embeddings_db\"\n",
    "        self.max_workers = 5\n",
    "        self.training_epochs = 30\n",
    "\n",
    "        # GPU Memory Management\n",
    "        self.gpu_memory_fraction = 0.9  \n",
    "        self.max_gpu_memory = \"32GiB\"    # Maximum GPU memory per device\n",
    "        self.cpu_offload_memory = \"24GiB\"  # CPU memory for offloading\n",
    "        \n",
    "        # Apply memory limits\n",
    "        self._setup_gpu_memory_limits()\n",
    "    \n",
    "    def _setup_gpu_memory_limits(self):\n",
    "        \"\"\"Setup GPU memory limits\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            # Clear any existing allocations first\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            \n",
    "            # Set much more conservative memory fraction\n",
    "            torch.cuda.set_per_process_memory_fraction(self.gpu_memory_fraction)\n",
    "            \n",
    "            # Set environment for memory management\n",
    "            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,expandable_segments:True'\n",
    "            \n",
    "            logger.info(f\"GPU memory STRICTLY limited to {self.gpu_memory_fraction*100}% of available memory\")\n",
    "\n",
    "# \n",
    "# LANGGRAPH STATE DEFINITION\n",
    "# \n",
    "\n",
    "# In the LANGGRAPH STATE DEFINITION section\n",
    "class FinancialState(TypedDict):\n",
    "    \"\"\"\n",
    "    The unified state for the financial agentic workflow.\n",
    "    \"\"\"\n",
    "    # Core message handling for the conversation\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    \n",
    "    # The agent responsible for all knowledge graph operations\n",
    "    kg_agent: Any\n",
    "    \n",
    "    # Results from the initial query analysis step\n",
    "    analysis_results: Dict[str, Any]\n",
    "    \n",
    "    # The specific financial entities resolved from the query\n",
    "    entities: List[Dict[str, Any]]\n",
    "    \n",
    "    # NEW: A dedicated key to hold insights from the GNN model\n",
    "    gnn_analysis: Dict[str, Any]\n",
    "\n",
    "\n",
    "def extract_json_from_response(text: str) -> Optional[Union[Dict, List]]:\n",
    "    # Preferred: within ``````\n",
    "    match = re.search(r\"``````\", text)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    # Fallback: first {...}\n",
    "    match = re.search(r\"(\\{[\\s\\S]+?\\})\", text)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    # Fallback: first [...] (for lists)\n",
    "    match = re.search(r\"(\\[[\\s\\S]+?\\])\", text)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    # Last resort: scan for first bracket, last closing bracket (your current logic)\n",
    "    try:\n",
    "        first_bracket_pos = min([p for p in [text.find('{'), text.find('[')] if p != -1], default=None)\n",
    "        last_bracket_pos = max(text.rfind('}'), text.rfind(']'))\n",
    "        if first_bracket_pos is not None and last_bracket_pos != -1:\n",
    "            potential_json = text[first_bracket_pos:last_bracket_pos+1]\n",
    "            return json.loads(potential_json)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# \n",
    "# ENHANCED SEC EDGAR DATA ENRICHMENT (From GNN Training Code)\n",
    "# \n",
    "\n",
    "\n",
    "class KaggleXBRLDataEnricher:\n",
    "    \"\"\"\n",
    "    ENHANCED: Handles a memory-efficient hybrid data enrichment stream with \n",
    "    robust caching, retry logic, and comprehensive error handling.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_agent=\"Financial Research Bot research@example.com\"):\n",
    "        self.user_agent = user_agent\n",
    "        self.api_cache = {}\n",
    "        # Debugging counters\n",
    "        self.other_count = 0\n",
    "        self.other_examples_shown = 0\n",
    "        self.sector_samples = {}\n",
    "        \n",
    "    def process_and_save_data(self, output_path: str, cache_file: str = \"./sec_api_cache.json\"):\n",
    "        \"\"\"\n",
    "        Main method to stream, enrich, and save company data to a .jsonl file.\n",
    "        \"\"\"\n",
    "        # Step 1 & 2: Identify missing CIKs and update the API cache\n",
    "        self._update_api_cache(cache_file)\n",
    "\n",
    "        # Step 3: Stream from files, enrich using the cache, and write to output\n",
    "        edgar_downloader = EDGARDataDownloader(user_agent=self.user_agent)\n",
    "        companyfacts_dir = \"edgar_data/companyfacts\"\n",
    "        data_generator = edgar_downloader.parse_company_facts(companyfacts_dir)\n",
    "        \n",
    "        processed_count = 0\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for company in data_generator:\n",
    "                enriched_company = self._enrich_single_company(company)\n",
    "                f.write(json.dumps(enriched_company) + '\\n')\n",
    "                processed_count += 1\n",
    "        \n",
    "        logger.info(f\" Saved {processed_count} enriched companies to '{output_path}'.\")\n",
    "\n",
    "    def _update_api_cache(self, cache_file: str):\n",
    "        \"\"\"\n",
    "        ENHANCED: Fetches missing CIKs in batches with exponential backoff retry logic\n",
    "        and saves progress incrementally to handle rate limits and interruptions gracefully.\n",
    "        \"\"\"\n",
    "        # Step 1: Load existing cache\n",
    "        try:\n",
    "            with open(cache_file, 'r') as f:\n",
    "                self.api_cache = json.load(f)\n",
    "            logger.info(f\"Loaded {len(self.api_cache)} CIKs from API cache '{cache_file}'.\")\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            self.api_cache = {}\n",
    "            logger.info(\"No existing API cache found. A new one will be created.\")\n",
    "\n",
    "        # Step 2: Identify all CIKs from the source files\n",
    "        edgar_downloader = EDGARDataDownloader(user_agent=self.user_agent)\n",
    "        companyfacts_dir = \"edgar_data/companyfacts\"\n",
    "        all_ciks = {c['cik'] for c in edgar_downloader.parse_company_facts(companyfacts_dir)}\n",
    "        \n",
    "        # Determine which CIKs are new and need to be fetched\n",
    "        ciks_to_fetch = sorted(list(all_ciks - set(self.api_cache.keys())))\n",
    "\n",
    "        if not ciks_to_fetch:\n",
    "            logger.info(\" API cache is already up-to-date.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\" Found {len(ciks_to_fetch)} new CIKs to fetch.\")\n",
    "\n",
    "        # Step 3: Fetch in batches and save progress incrementally\n",
    "        batch_size = 500\n",
    "        for i in range(0, len(ciks_to_fetch), batch_size):\n",
    "            batch = ciks_to_fetch[i:i+batch_size]\n",
    "            logger.info(f\" Processing batch {i//batch_size + 1}/{-(-len(ciks_to_fetch)//batch_size)} (CIKs {i} to {i+len(batch)-1}) \")\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                # Map each future to its CIK\n",
    "                future_to_cik = {executor.submit(self._fetch_sec_data_for_cik, cik): cik for cik in batch}\n",
    "                \n",
    "                # Process results as they complete\n",
    "                for future in as_completed(future_to_cik):\n",
    "                    cik = future_to_cik[future]\n",
    "                    result = future.result()\n",
    "                    if result:  # Only add to cache if the fetch was successful\n",
    "                        self.api_cache[cik] = result\n",
    "            \n",
    "            # Save progress after each batch\n",
    "            try:\n",
    "                with open(cache_file, 'w') as f:\n",
    "                    json.dump(self.api_cache, f)\n",
    "                logger.info(f\" Batch complete. API cache now has {len(self.api_cache)} entries.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Could not save API cache to '{cache_file}': {e}\")\n",
    "\n",
    "    def _fetch_sec_data_for_cik(self, cik: str) -> Optional[Dict]:\n",
    "        \"\"\"Enhanced fetch with exponential backoff retry mechanism\"\"\"\n",
    "        if not cik or not str(cik).isdigit(): \n",
    "            return None\n",
    "        \n",
    "        url = f\"https://data.sec.gov/submissions/CIK{str(cik).zfill(10)}.json\"\n",
    "        headers = {'User-Agent': self.user_agent}\n",
    "\n",
    "        # Enhanced retry mechanism with exponential backoff\n",
    "        max_retries = 5\n",
    "        base_delay = 1  # Start with a 1-second delay\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()  # Will raise an exception for 4xx/5xx errors\n",
    "                \n",
    "                # Success! Pause slightly to respect rate limits even on success.\n",
    "                time.sleep(0.9) \n",
    "                return response.json()\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # Specifically check for the \"Too Many Requests\" error\n",
    "                if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:\n",
    "                    delay = base_delay * (2 ** attempt) + np.random.uniform(0, 1)\n",
    "                    logger.warning(\n",
    "                        f\"Rate limit exceeded for CIK {cik}. \"\n",
    "                        f\"Retrying in {delay:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\"\n",
    "                    )\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    # For other errors (e.g., 404 Not Found), don't retry.\n",
    "                    logger.warning(f\"Failed to fetch data for CIK {cik}. Reason: {e}\")\n",
    "                    return None  # Stop trying for this CIK\n",
    "        \n",
    "        # If all retries fail, log it and give up.\n",
    "        logger.error(f\"All {max_retries} retries failed for CIK {cik}. Giving up.\")\n",
    "        return None\n",
    "\n",
    "    def _enrich_single_company(self, base_company: Dict) -> Dict:\n",
    "        \"\"\"Enriches a single company record with API data and a standardized sector.\"\"\"\n",
    "        api_data = self.api_cache.get(base_company['cik'])\n",
    "        \n",
    "        # Start with the base data from the file\n",
    "        final_data = base_company\n",
    "        \n",
    "        # If API data exists, intelligently merge it, preferring API values\n",
    "        # but falling back to the base data if a key is missing from the API response.\n",
    "        if api_data:\n",
    "            final_data = {\n",
    "                'cik': base_company['cik'],  # Keep the original CIK\n",
    "                'entityName': api_data.get('name') or base_company.get('entityName'),\n",
    "                'tickers': api_data.get('tickers') or base_company.get('tickers', []),\n",
    "                'exchanges': api_data.get('exchanges') or base_company.get('exchanges', []),\n",
    "                'sic': api_data.get('sic') or base_company.get('sic'),  \n",
    "                'sicDescription': api_data.get('sicDescription') or base_company.get('sicDescription'),\n",
    "                'stateOfIncorporation': api_data.get('stateOfIncorporation') or base_company.get('stateOfIncorporation'),\n",
    "                'fiscalYearEnd': api_data.get('fiscalYearEnd') or base_company.get('fiscalYearEnd'),\n",
    "                'facts': base_company.get('facts', {})  # Always keep the detailed facts from the file\n",
    "            }\n",
    "\n",
    "        # Enhanced sector mapping\n",
    "        sector = 'Other'\n",
    "        sic_code_str = final_data.get('sic')\n",
    "        if sic_code_str and str(sic_code_str).isdigit():\n",
    "            sector = self._sic_to_sector(int(sic_code_str))\n",
    "        \n",
    "        # Collect sector samples for debugging\n",
    "        if sector not in self.sector_samples:\n",
    "            self.sector_samples[sector] = []\n",
    "        \n",
    "        if len(self.sector_samples[sector]) < 5:\n",
    "            sample_info = f\"Name: '{final_data.get('entityName', 'N/A')}', SIC: {sic_code_str}\"\n",
    "            self.sector_samples[sector].append(sample_info)\n",
    "\n",
    "        final_data['market_data'] = {\n",
    "            'sector': sector, \n",
    "            'industry': final_data.get('sicDescription', ''),\n",
    "            'data_source': 'edgar_hybrid_enhanced'\n",
    "        }\n",
    "        return final_data\n",
    "        \n",
    "    def _sic_to_sector(self, sic_code: int) -> str:\n",
    "        \"\"\"Enhanced SIC to sector mapping with more comprehensive classifications\"\"\"\n",
    "        if not sic_code or not 100 <= sic_code <= 9999: \n",
    "            return 'Other'\n",
    "        \n",
    "        # Special cases first\n",
    "        if 2836 == sic_code: return 'Biotechnology'\n",
    "        if 5961 == sic_code: return 'E-commerce'\n",
    "        \n",
    "        # Standard SIC ranges\n",
    "        if 100 <= sic_code <= 999: return 'Agriculture'\n",
    "        if 1000 <= sic_code <= 1499: return 'Mining'\n",
    "        if 1500 <= sic_code <= 1799: return 'Construction'\n",
    "        \n",
    "        if 2000 <= sic_code <= 3999:\n",
    "            # Technology manufacturing subcategories\n",
    "            tech_mfg = [(3570, 3579), (3600, 3699), (3812, 3812), (3823, 3829), (3840, 3849)]\n",
    "            if any(s <= sic_code <= e for s, e in tech_mfg): \n",
    "                return 'Technology'\n",
    "            # Healthcare/biotech manufacturing\n",
    "            if 2834 <= sic_code <= 2836: \n",
    "                return 'Healthcare'\n",
    "            return 'Manufacturing'\n",
    "        \n",
    "        if 4000 <= sic_code <= 4999:\n",
    "            # Telecom/tech services\n",
    "            if 4800 <= sic_code <= 4899: \n",
    "                return 'Technology'\n",
    "            return 'Transportation & Public Utilities'\n",
    "        \n",
    "        if 5000 <= sic_code <= 5199: return 'Wholesale Trade'\n",
    "        if 5200 <= sic_code <= 5999: return 'Retail Trade'\n",
    "        if 6000 <= sic_code <= 6799: return 'Financial Services'\n",
    "        \n",
    "        if 7000 <= sic_code <= 8999:\n",
    "            # Technology services subcategories\n",
    "            tech_svc = [(7370, 7379), (7380, 7389), (8711, 8711), (8748, 8748)]\n",
    "            if any(s <= sic_code <= e for s, e in tech_svc): \n",
    "                return 'Technology'\n",
    "            # Healthcare services\n",
    "            if 8000 <= sic_code <= 8099: \n",
    "                return 'Healthcare'\n",
    "            # Entertainment & Media\n",
    "            ent_media = [(7800, 7841), (4830, 4841)]\n",
    "            if any(s <= sic_code <= e for s, e in ent_media): \n",
    "                return 'Entertainment & Media'\n",
    "            return 'Services'\n",
    "        \n",
    "        if 9100 <= sic_code <= 9729: return 'Public Administration'\n",
    "        return 'Other'\n",
    "\n",
    "    def enrich_companies_parallel(self, companies_data, max_workers=3):\n",
    "        \"\"\"Legacy method maintained for compatibility\"\"\"\n",
    "        logger.info(\"Using enhanced KaggleXBRLDataEnricher for parallel enrichment...\")\n",
    "        return self.process_and_save_data(\"./enriched_companies.jsonl\")\n",
    "\n",
    "    def fetch_edgar_company_data(self, cik):\n",
    "        \"\"\"Legacy method wrapper for compatibility\"\"\"\n",
    "        return self._fetch_sec_data_for_cik(str(cik))\n",
    "\n",
    "class SECAPITool:\n",
    "    \"\"\"Enhanced SEC API Tool with Yahoo Finance fallback\"\"\"\n",
    "\n",
    "    def __init__(self, user_agent=\"Financial Research Bot research@example.com\"):\n",
    "        self.user_agent = user_agent\n",
    "        self.headers = {\"User-Agent\": self.user_agent}\n",
    "        # Cache for company name to ticker mappings\n",
    "        self.ticker_cache = {}\n",
    "        \n",
    "    def get_ticker_from_company_name(self, company_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Convert company name to ticker symbol using Yahoo Finance Search API\n",
    "        \"\"\"\n",
    "        # Check cache first\n",
    "        if company_name in self.ticker_cache:\n",
    "            return self.ticker_cache[company_name]\n",
    "            \n",
    "        try:\n",
    "            url = \"https://query2.finance.yahoo.com/v1/finance/search\"\n",
    "            \n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            params = {\n",
    "                \"q\": company_name,\n",
    "                \"quotes_count\": 1,\n",
    "                \"country\": \"United States\"\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url=url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract ticker from response\n",
    "            if data.get('quotes') and len(data['quotes']) > 0:\n",
    "                ticker = data['quotes'][0]['symbol']\n",
    "                self.ticker_cache[company_name] = ticker  # Cache the result\n",
    "                return ticker\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting ticker for {company_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def get_yahoo_finance_data(self, company_name: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Get comprehensive financial data from Yahoo Finance\n",
    "        \"\"\"\n",
    "        # First, get the ticker symbol\n",
    "        ticker = self.get_ticker_from_company_name(company_name)\n",
    "        if not ticker:\n",
    "            logger.info(f\" ticker not found for {company_name}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Create yfinance Ticker object\n",
    "            logger.info(f\" ticker {ticker} found for {company_name}\")\n",
    "            stock = yf.Ticker(ticker)\n",
    "            info = stock.info\n",
    "            \n",
    "            if not info:\n",
    "                return None\n",
    "                \n",
    "            # Extract key financial metrics\n",
    "            financial_data = {\n",
    "                \"entityName\": info.get('longName') or info.get('shortName') or company_name,\n",
    "                \"ticker\": ticker,\n",
    "                \"sector\": info.get('sector', 'N/A'),\n",
    "                \"industry\": info.get('industry', 'N/A'),\n",
    "                \n",
    "                # Valuation Ratios\n",
    "                \"P/E Ratio\": info.get('trailingPE'),\n",
    "                \"Forward P/E\": info.get('forwardPE'), \n",
    "                \"P/B Ratio\": info.get('priceToBook'),\n",
    "                \"P/S Ratio\": info.get('priceToSalesTrailing12Months'),\n",
    "                \"PEG Ratio\": info.get('pegRatio'),\n",
    "                \n",
    "                # Market Data\n",
    "                \"Market Cap\": info.get('marketCap'),\n",
    "                \"Current Price\": info.get('currentPrice') or info.get('regularMarketPrice'),\n",
    "                \"52 Week High\": info.get('fiftyTwoWeekHigh'),\n",
    "                \"52 Week Low\": info.get('fiftyTwoWeekLow'),\n",
    "                \n",
    "                # Financial Metrics\n",
    "                \"Revenue\": info.get('totalRevenue'),\n",
    "                \"Revenue Per Share\": info.get('revenuePerShare'),\n",
    "                \"Total Cash\": info.get('totalCash'),\n",
    "                \"Total Debt\": info.get('totalDebt'),\n",
    "                \"Book Value\": info.get('bookValue'),\n",
    "                \"Enterprise Value\": info.get('enterpriseValue'),\n",
    "                \n",
    "                # Profitability\n",
    "                \"Profit Margins\": info.get('profitMargins'),\n",
    "                \"Operating Margins\": info.get('operatingMargins'),\n",
    "                \"ROE\": info.get('returnOnEquity'),\n",
    "                \"ROA\": info.get('returnOnAssets'),\n",
    "                \n",
    "                # Dividend Info\n",
    "                \"Dividend Yield\": info.get('dividendYield'),\n",
    "                \"Dividend Rate\": info.get('dividendRate'),\n",
    "                \n",
    "                # Growth & Performance\n",
    "                \"Revenue Growth\": info.get('revenueGrowth'),\n",
    "                \"Earnings Growth\": info.get('earningsGrowth'),\n",
    "                \"Beta\": info.get('beta'),\n",
    "                \n",
    "                # Share Information\n",
    "                \"Shares Outstanding\": info.get('sharesOutstanding'),\n",
    "                \"Float\": info.get('floatShares'),\n",
    "                \n",
    "                \"data_source\": \"yahoo_finance\"\n",
    "            }\n",
    "            \n",
    "            # Clean up None values and format numbers\n",
    "            cleaned_data = {}\n",
    "            for key, value in financial_data.items():\n",
    "                if value is not None and key != \"data_source\":\n",
    "                    if isinstance(value, (int, float)) and abs(value) > 1000000:\n",
    "                        # Keep large numbers as is for calculations\n",
    "                        cleaned_data[key] = value\n",
    "                    else:\n",
    "                        cleaned_data[key] = value\n",
    "                elif key in [\"entityName\", \"ticker\", \"sector\", \"industry\", \"data_source\"]:\n",
    "                    cleaned_data[key] = value or \"N/A\"\n",
    "                   \n",
    "            return cleaned_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Yahoo Finance API failed for {company_name} ({ticker}): {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_live_company_data(self, cik: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        ENHANCED: Try SEC first, then fallback to Yahoo Finance\n",
    "        \"\"\"\n",
    "        # Step 1: Try SEC EDGAR API first\n",
    "        sec_data = super().get_live_company_data(cik)\n",
    "        \n",
    "        if sec_data and any(isinstance(v, (int, float)) and v > 0 for v in sec_data.values() if v):\n",
    "            logger.info(f\" Successfully fetched SEC data for CIK {cik}\")\n",
    "            return sec_data\n",
    "            \n",
    "        # Step 2: Fallback to Yahoo Finance using company name lookup\n",
    "        logger.info(f\"SEC data insufficient for CIK {cik}. Trying Yahoo Finance fallback...\")\n",
    "        \n",
    "        # Try to get company name from SEC first\n",
    "        company_name = None\n",
    "        if sec_data and sec_data.get('entityName'):\n",
    "            company_name = sec_data['entityName']\n",
    "        else:\n",
    "            # Try a basic SEC submission call to get just the company name\n",
    "            try:\n",
    "                formatted_cik = str(cik).zfill(10)\n",
    "                submissions_url = f\"https://data.sec.gov/submissions/CIK{formatted_cik}.json\"\n",
    "                response = requests.get(submissions_url, headers=self.headers)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    company_name = data.get('name')\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        if company_name:\n",
    "            yahoo_data = self.get_yahoo_finance_data(company_name)\n",
    "            if yahoo_data:\n",
    "                logger.info(f\" Successfully fetched Yahoo Finance data for {company_name}\")\n",
    "                return yahoo_data\n",
    "                \n",
    "        logger.warning(f\"Both SEC and Yahoo Finance failed for CIK {cik}\")\n",
    "        return None\n",
    "        \n",
    "# \n",
    "# PHI-4 FINE-TUNED MODEL HANDLER\n",
    "# \n",
    "\n",
    "class FineTunedPhi4Agent:\n",
    "    \"\"\"Handler for fine-tuned Phi-4 model\"\"\"\n",
    "\n",
    "    def __init__(self, config: SystemConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.load_model()\n",
    "\n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Counts the actual number of tokens in a string.\"\"\"\n",
    "        if not self.tokenizer:\n",
    "            return len(text) // 4 # Fallback estimation\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the fine-tuned Phi-4 model\"\"\"\n",
    "        try:\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.fine_tuned_model_path)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            # Load base model first\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.phi4_model_path,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                use_cache=True,\n",
    "                offload_folder=\"./offload\",\n",
    "            )\n",
    "\n",
    "            # Try to load PEFT adapters\n",
    "            try:\n",
    "                self.model = PeftModel.from_pretrained(\n",
    "                    base_model,\n",
    "                    self.config.fine_tuned_model_path,\n",
    "                    offload_folder=\"./offload\"\n",
    "                )\n",
    "                logger.info(\"Fine-tuned Phi-4 model loaded successfully\")\n",
    "            except:\n",
    "                self.model = base_model\n",
    "                logger.info(\"Base Phi-4 model loaded (fine-tuned version unavailable)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {e}\")\n",
    "            # Use a placeholder model\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 150, temperature: float = 0.7, do_sample: bool = True) -> str:\n",
    "        \"\"\"Generate response using the model with proper tokenization handling.\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return f\"Model unavailable. Response for: {prompt[:100]}...\"\n",
    "    \n",
    "        # Build messages in training format\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial assistant. Reason step-by-step to answer the user's question based on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    \n",
    "        # Apply chat template\n",
    "        try:\n",
    "            final_prompt = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Chat template failed, using fallback: {e}\")\n",
    "            final_prompt = f\"<|system|>\\nYou are a financial assistant.\\n<|end|>\\n<|user|>\\n{prompt}\\n<|end|>\\n<|assistant|>\\n\"\n",
    "    \n",
    "        # Tokenize with proper settings\n",
    "        inputs = self.tokenizer(\n",
    "            final_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=self.config.max_sequence_length - max_new_tokens,\n",
    "            padding=False\n",
    "        )\n",
    "    \n",
    "        # Move to device\n",
    "        device = next(self.model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "        # Generate with proper stopping criteria\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature if do_sample else 1.0,\n",
    "                    do_sample=do_sample,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2,\n",
    "                    num_return_sequences=1,\n",
    "                    # Remove early_stopping=True as it's not valid for this model\n",
    "                    use_cache=True\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation failed: {e}\")\n",
    "            return \"I apologize, but I encountered an error generating the response.\"\n",
    "    \n",
    "        # Correct slicing - use shape[1] not shape[26]\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        new_tokens = outputs[0][input_length:]  \n",
    "        \n",
    "        try:\n",
    "            response = self.tokenizer.decode(\n",
    "                new_tokens, \n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Decoding failed: {e}\")\n",
    "            return \"I apologize, but I encountered an error decoding the response.\"\n",
    "    \n",
    "        # Clean up artifacts\n",
    "        response = response.replace(\"<|end|>\", \"\").replace(\"<|assistant|>\", \"\")\n",
    "        \n",
    "        # Fix character ladder issue by removing excessive newlines\n",
    "        response = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', response)  # Max 2 consecutive newlines\n",
    "        response = re.sub(r'(\\w)\\n(\\w)', r'\\1 \\2', response)    # Fix broken words\n",
    "        \n",
    "        return response.strip()\n",
    "\n",
    "                 \n",
    "\n",
    "    def extract_claims(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract factual claims from text for verification\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Extract specific factual claims from this financial text:\n",
    "        Text: {text}\n",
    "        \n",
    "        Return only concrete, verifiable claims as a JSON list:\n",
    "        [\"claim1\", \"claim2\", \"claim3\"]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.generate(prompt, max_new_tokens=200, temperature=0.1)\n",
    "        \n",
    "        claims = extract_json_from_response(response)\n",
    "        if claims and isinstance(claims, list):\n",
    "            return claims\n",
    "        else:\n",
    "            # Fallback: simple sentence splitting\n",
    "            sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 10]\n",
    "            return sentences[:5]\n",
    "\n",
    "    def analyze_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced query analysis with better routing logic\"\"\"\n",
    "        \n",
    "        strict_prompt = f\"\"\"You are an expert financial query analyzer. Your task is to analyze the user's query and return a JSON object with the query type and any identified financial entities. Your response must be ONLY the JSON object.\n",
    "    \n",
    "        **Rules:**\n",
    "        - **query_type**: Must be one of \"local\", \"global\", or \"numerical\".\n",
    "        - **Query Types Defined:**\n",
    "          - \"local\": Finding similar companies, comparisons between specific entities\n",
    "          - \"global\": Asking for specific financial data/ratios of a single company, general market questions\n",
    "          - \"numerical\": Mathematical calculations with specific numbers provided in query\n",
    "        - **entities**: A list of company names. Always include entities even for \"global\" queries if companies are mentioned.\n",
    "        \n",
    "        **Examples:**\n",
    "        - Query: \"Find companies similar to Apple Inc\" → {{\"query_type\": \"local\", \"entities\": [\"Apple Inc\"]}}\n",
    "        - Query: \"What is the P/E ratio of Apple?\" → {{\"query_type\": \"global\", \"entities\": [\"Apple Inc\"]}}\n",
    "        - Query: \"What are the top tech companies?\" → {{\"query_type\": \"global\", \"entities\": []}}\n",
    "        - Query: \"If a company has $500 in revenue and $100 in profit, what is the profit margin?\" → {{\"query_type\": \"numerical\", \"entities\": []}}\n",
    "    \n",
    "        **User Query to Analyze:**\n",
    "        \"{query}\"\n",
    "        \n",
    "        **JSON Output:**\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.generate(strict_prompt, max_new_tokens=100, temperature=0.1)\n",
    "            analysis = extract_json_from_response(response)\n",
    "            \n",
    "            if analysis and \"query_type\" in analysis and \"entities\" in analysis:\n",
    "                return analysis\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Strict JSON prompt failed: {e}\")\n",
    "        \n",
    "        # Pattern-based fallback with better logic\n",
    "        return self._pattern_based_analysis_enhanced(query)\n",
    "    \n",
    "    def _pattern_based_analysis_enhanced(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced pattern-based analysis with better query type detection\"\"\"\n",
    "        \n",
    "        # Extract entities\n",
    "        company_pattern = r'\\b(?:[A-Z][A-Za-z-&\\'\\.]+\\s?)+(?:Inc|Corp|LLC|Ltd|Co|Group|PLC)?\\.?\\b'\n",
    "        ticker_pattern = r'\\b([A-Z]{2,5})\\b'\n",
    "        \n",
    "        entities = re.findall(company_pattern, query, re.IGNORECASE)\n",
    "        entities.extend(m for m in re.findall(ticker_pattern, query) if len(m) > 1)\n",
    "        \n",
    "        # Filter stopwords\n",
    "        stopwords = {\"Find\", \"Compare\", \"What\", \"Show\", \"The\", \"Who\", \"When\", \"Where\", \"Why\", \"How\", \"Companies\", \"That\", \"Are\"}\n",
    "        final_entities = [e.strip() for e in entities if e.strip().title() not in stopwords]\n",
    "        \n",
    "        # Better query type detection\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Check for similarity queries FIRST (highest priority)\n",
    "        competitor_patterns = [\n",
    "            'similar', 'like', 'competitors', 'compete', 'peers', 'vs', \n",
    "            'comparable', 'rivalry', 'alternative', 'substitute'\n",
    "        ]\n",
    "        if any(pattern in query_lower for pattern in competitor_patterns):\n",
    "            return {\"query_type\": \"local\", \"entities\": final_entities}\n",
    "        \n",
    "        # Check for numerical patterns\n",
    "        if any(calc_word in query_lower for calc_word in ['calculate', 'compute', 'if', 'given that', '%', 'margin']):\n",
    "            if re.search(r'\\$?\\d+', query):  # Contains specific numbers\n",
    "                return {\"query_type\": \"numerical\", \"entities\": final_entities}\n",
    "        \n",
    "        # Check for specific data requests about entities\n",
    "        if final_entities and any(data_word in query_lower for data_word in ['ratio', 'price', 'revenue', 'income', 'assets', 'what is']):\n",
    "            return {\"query_type\": \"global\", \"entities\": final_entities}\n",
    "        \n",
    "        # Default logic\n",
    "        query_type = \"local\" if final_entities else \"global\"\n",
    "        \n",
    "        return {\n",
    "            \"query_type\": query_type,\n",
    "            \"entities\": final_entities,\n",
    "            \"intent\": \"search\",\n",
    "            \"complexity\": 0.5,\n",
    "            \"extraction_method\": \"enhanced_pattern_based_fallback\"\n",
    "        }\n",
    "\n",
    "\n",
    "    \n",
    "    def _extract_json_with_multiple_strategies(self, response: str) -> Dict[str, Any]:\n",
    "        \"\"\"Multiple JSON extraction strategies\"\"\"\n",
    "        \n",
    "        strategies = [\n",
    "            # Strategy 1: Direct JSON parsing\n",
    "            lambda r: json.loads(r.strip()),\n",
    "            \n",
    "            # Strategy 2: Regex extraction\n",
    "            lambda r: json.loads(re.search(r'\\{.*\\}', r, re.DOTALL).group()),\n",
    "            \n",
    "            # Strategy 3: Line-by-line parsing\n",
    "            lambda r: json.loads('\\n'.join([line.strip() for line in r.split('\\n') if line.strip().startswith('{')][0])),\n",
    "            \n",
    "            # Strategy 4: Remove common prefixes\n",
    "            lambda r: json.loads(re.sub(r'^[^{]*', '', r).split('\\n')[0])\n",
    "        ]\n",
    "        \n",
    "        for i, strategy in enumerate(strategies):\n",
    "            try:\n",
    "                result = strategy(response)\n",
    "                if isinstance(result, dict):\n",
    "                    logger.info(f\"JSON extraction succeeded with strategy {i+1}\")\n",
    "                    return result\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "    \n",
    "    def _validate_analysis_structure(self, analysis: Dict) -> bool:\n",
    "        \"\"\"Validate that analysis has required structure\"\"\"\n",
    "        if not analysis or not isinstance(analysis, dict):\n",
    "            return False\n",
    "        \n",
    "        required_keys = [\"intent\", \"entities\", \"complexity\"]\n",
    "        return all(key in analysis for key in required_keys)\n",
    "\n",
    "# \n",
    "# GRAPHSAGE EMBEDDINGS AND TRAINING\n",
    "# \n",
    "\n",
    "class MultilingualE5Embedder:\n",
    "    \"\"\"\n",
    "    Multilingual-E5-Large embedder using transformers library\n",
    "    Replaces SentenceTransformer with better performance and 1024-dimensional outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='intfloat/multilingual-e5-large', device=None):\n",
    "        self.model_name = model_name\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,  # Use fp16 to save memory\n",
    "        ).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        logger.info(f\"Loaded {model_name} on {self.device}\")\n",
    "    \n",
    "    def average_pool(self, last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        \"\"\"Average pooling for sentence embeddings\"\"\"\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    \n",
    "    def format_text_with_prefix(self, text: str, text_type: str = \"passage\") -> str:\n",
    "        \"\"\"Format text with appropriate E5 prefixes for optimal performance\"\"\"\n",
    "        if text_type == \"query\":\n",
    "            return f\"query: {text}\"\n",
    "        elif text_type == \"passage\":\n",
    "            return f\"passage: {text}\"\n",
    "        else:\n",
    "            return f\"passage: {text}\"  # Default to passage\n",
    "    \n",
    "    def encode(self, texts: Union[str, List[str]], \n",
    "               text_type: str = \"passage\", \n",
    "               normalize: bool = True, \n",
    "               batch_size: int = 32,\n",
    "               show_progress: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode texts to embeddings with proper E5 prefixes\n",
    "        \"\"\"\n",
    "        \n",
    "        # Handle single text input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Handle empty text lists\n",
    "        if not texts:\n",
    "            logger.warning(\"Empty text list provided to encoder. Returning empty array.\")\n",
    "            return np.empty((0, 1024), dtype=np.float32)  # Return empty array with correct shape\n",
    "        \n",
    "        # Format texts with prefixes\n",
    "        formatted_texts = [self.format_text_with_prefix(text, text_type) for text in texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(formatted_texts), batch_size):\n",
    "            batch_texts = formatted_texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            batch_dict = self.tokenizer(\n",
    "                batch_texts, \n",
    "                max_length=512,  # E5 model max length\n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            batch_dict = {k: v.to(self.device) for k, v in batch_dict.items()}\n",
    "            \n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**batch_dict)\n",
    "                embeddings = self.average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "                \n",
    "                if normalize:\n",
    "                    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "                \n",
    "                all_embeddings.append(embeddings.cpu())\n",
    "            \n",
    "            if show_progress and i % (batch_size * 10) == 0:\n",
    "                logger.info(f\"Processed {i + len(batch_texts)}/{len(formatted_texts)} texts\")\n",
    "        \n",
    "        # Handle empty embeddings list\n",
    "        if not all_embeddings:\n",
    "            logger.warning(\"No embeddings generated. Returning empty array.\")\n",
    "            return np.empty((0, 1024), dtype=np.float32)\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        final_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        return final_embeddings.to(torch.float32).numpy()\n",
    "\n",
    "    \n",
    "    def encode_queries(self, queries: Union[str, List[str]], **kwargs) -> np.ndarray:\n",
    "        \"\"\"Encode queries with query prefix\"\"\"\n",
    "        return self.encode(queries, text_type=\"query\", **kwargs)\n",
    "    \n",
    "    def encode_passages(self, passages: Union[str, List[str]], **kwargs) -> np.ndarray:\n",
    "        \"\"\"Encode passages with passage prefix\"\"\"\n",
    "        return self.encode(passages, text_type=\"passage\", **kwargs)\n",
    "\n",
    "\n",
    "class EnhancedFinancialKnowledgeGraph:\n",
    "    \"\"\"\n",
    "    REFACTORED: This version is designed to be built entirely from a data stream\n",
    "    to handle datasets that are larger than available RAM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.entity_attributes = {}\n",
    "        self.relationship_types = set()\n",
    "        self.feature_names = [\n",
    "            'type_company', 'type_metric', 'type_industry', 'type_market_metric',\n",
    "            'total_assets', 'revenues', 'net_income', 'stockholders_equity',\n",
    "            'mkt_current_price', 'mkt_market_cap', 'mkt_pe_ratio', 'mkt_pb_ratio',\n",
    "            'mkt_dividend_yield', 'mkt_beta', 'mkt_return_on_equity', 'mkt_debt_to_equity',\n",
    "            'sic_numeric', 'num_exchanges', 'num_tickers'\n",
    "        ]\n",
    "        self.feature_map = {name: i for i, name in enumerate(self.feature_names)}\n",
    "        self.industry_to_companies = {}\n",
    "\n",
    "    def build_graph_from_stream(self, companies_data_generator: iter):\n",
    "        \"\"\"\n",
    "        NEW: Builds the graph in two passes directly from the stream.\n",
    "        Pass 1 adds all nodes. Pass 2 adds complex edges that require global context.\n",
    "        \"\"\"\n",
    "        #  PASS 1: Add all nodes and their attributes from the stream \n",
    "        logger.info(\"Building graph - Pass 1/2: Streaming and adding all nodes...\")\n",
    "        for company in companies_data_generator:\n",
    "            self._add_company_node(company)\n",
    "            # Also add simple, single-company relationships during this pass\n",
    "            self._add_financial_relationships(company)\n",
    "            self._add_industry_relationships(company)\n",
    "            self._add_market_relationships(company)\n",
    "        \n",
    "        logger.info(f\"Pass 1 complete. Graph has {self.graph.number_of_nodes()} nodes.\")\n",
    "\n",
    "        #  PASS 2: Add relationships that require all nodes to be present \n",
    "        logger.info(\"Building graph - Pass 2/2: Adding peer and market cap relationships...\")\n",
    "        self._group_companies_by_industry_from_graph() # This now works on the graph itself\n",
    "        self._add_peer_relationships()\n",
    "        self._add_market_cap_relationships()\n",
    "        \n",
    "        logger.info(f\"Built graph with {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges.\")\n",
    "        self._analyze_graph_connectivity()\n",
    "\n",
    "    def _group_companies_by_industry_from_graph(self):\n",
    "        \"\"\"NEW: Groups companies by iterating over the graph nodes, not a list.\"\"\"\n",
    "        logger.info(\"Grouping companies by sector directly from graph nodes...\")\n",
    "        for node_id, attrs in self.graph.nodes(data=True):\n",
    "            if attrs.get('type') == 'company':\n",
    "                sector = attrs.get('sector', 'Other')\n",
    "                if sector not in self.industry_to_companies:\n",
    "                    self.industry_to_companies[sector] = []\n",
    "                self.industry_to_companies[sector].append(node_id)\n",
    "        \n",
    "        total_grouped = sum(len(c) for c in self.industry_to_companies.values())\n",
    "        logger.info(f\"Grouped {total_grouped} companies into {len(self.industry_to_companies)} sectors.\")\n",
    "    \n",
    "    def _analyze_graph_connectivity(self):\n",
    "        \"\"\"Analyze and report graph connectivity\"\"\"\n",
    "        \n",
    "        if self.graph.number_of_nodes() == 0:\n",
    "            logger.warning(\"Graph is empty!\")\n",
    "            return\n",
    "        \n",
    "        # Convert to undirected for connectivity analysis\n",
    "        undirected = self.graph.to_undirected()\n",
    "        \n",
    "        # Find connected components\n",
    "        components = list(nx.connected_components(undirected))\n",
    "        largest_component = max(components, key=len) if components else set()\n",
    "        \n",
    "        logger.info(f\"Graph connectivity analysis:\")\n",
    "        logger.info(f\"  Total nodes: {self.graph.number_of_nodes()}\")\n",
    "        logger.info(f\"  Total edges: {self.graph.number_of_edges()}\")\n",
    "        logger.info(f\"  Connected components: {len(components)}\")\n",
    "        logger.info(f\"  Largest component size: {len(largest_component)} ({len(largest_component)/self.graph.number_of_nodes()*100:.1f}%)\")\n",
    "        \n",
    "        if len(components) > 1:\n",
    "            logger.warning(f\"Graph has {len(components)} disconnected components!\")\n",
    "            component_sizes = sorted([len(comp) for comp in components], reverse=True)\n",
    "            logger.info(f\"  Component sizes: {component_sizes[:10]}\")  # Show top 10\n",
    "\n",
    "            \n",
    "    def _add_company_node(self, company):\n",
    "        \"\"\"\n",
    "        MODIFIED: Adds a company node but no longer stores the huge raw 'facts' \n",
    "        dictionary as a node attribute to save significant memory.\n",
    "        \"\"\"\n",
    "        cik = str(company['cik'])\n",
    "        entity_name = company['entityName']\n",
    "        market_data = company.get('market_data', {})\n",
    "        \n",
    "        # This is the raw financial data, which can be very large\n",
    "        facts = company.get('facts', {})\n",
    "        \n",
    "        # We extract the necessary numerical metrics from 'facts'\n",
    "        financial_attrs = self._extract_financial_metrics(facts)\n",
    "        market_attrs = self._process_market_attributes(market_data)\n",
    "\n",
    "        node_attrs = {\n",
    "            'type': 'company',\n",
    "            'name': entity_name,\n",
    "            'cik': cik,\n",
    "            'sector': market_data.get('sector', 'Other'), \n",
    "            'tickers': company.get('tickers', []),\n",
    "            'sic': company.get('sic', ''),\n",
    "            'sic_description': company.get('sicDescription', ''),\n",
    "            'state': company.get('stateOfIncorporation', ''),\n",
    "            'fiscal_year_end': company.get('fiscalYearEnd', ''),\n",
    "            'exchanges': company.get('exchanges', [])\n",
    "        }\n",
    "        \n",
    "        # Add the extracted financial and market metrics\n",
    "        node_attrs.update(financial_attrs)\n",
    "        node_attrs.update(market_attrs)\n",
    "        \n",
    "        # CRITICAL CHANGE: We DO NOT add the raw 'facts' object to the node.\n",
    "        # The 'financial_attrs' already contain everything we need for GNN features.\n",
    "        \n",
    "        self.graph.add_node(cik, **node_attrs)\n",
    "        self.entity_attributes[cik] = node_attrs\n",
    "\n",
    "\n",
    "    def _add_peer_relationships(self):\n",
    "        \"\"\"\n",
    "        Adds peer relationships using a capped random sampling strategy\n",
    "        for ALL sectors to prevent combinatorial explosion and over-density.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Creating peer-to-peer edges for {len(self.industry_to_companies)} industries...\")\n",
    "        edge_count = 0\n",
    "        # Set a hard limit on connections per company to keep the graph sparse\n",
    "        max_connections_per_company = 15 \n",
    "        \n",
    "        import random\n",
    "        \n",
    "        for sector, companies in self.industry_to_companies.items():\n",
    "            if len(companies) <= 1:\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"  Processing {sector} with {len(companies)} companies\")\n",
    "            \n",
    "            # Apply the same safe, sampling logic to every sector\n",
    "            for company in companies:\n",
    "                # Find potential peers (all other companies in the sector)\n",
    "                peers = [c for c in companies if c != company]\n",
    "                \n",
    "                # Determine how many connections to make\n",
    "                num_to_connect = min(max_connections_per_company, len(peers))\n",
    "                selected_peers = random.sample(peers, num_to_connect)\n",
    "                \n",
    "                for peer in selected_peers:\n",
    "                    # Add a single directed edge (will be treated as undirected by GNNs later)\n",
    "                    if not self.graph.has_edge(company, peer):\n",
    "                        self.graph.add_edge(company, peer, relationship='peer_of')\n",
    "                        edge_count += 1\n",
    "                        \n",
    "        self.relationship_types.add('peer_of')\n",
    "        logger.info(f\"Added {edge_count} 'peer_of' edges to the graph.\")\n",
    "\n",
    "    \n",
    "    def _add_market_cap_relationships(self):\n",
    "        \"\"\"Add relationships between companies of similar market cap\"\"\"\n",
    "        \n",
    "        logger.info(\"Adding market cap similarity relationships...\")\n",
    "        \n",
    "        # Group companies by market cap ranges\n",
    "        cap_ranges = {\n",
    "            'mega_cap': [],    # > 200B\n",
    "            'large_cap': [],   # 10B - 200B  \n",
    "            'mid_cap': [],     # 2B - 10B\n",
    "            'small_cap': []    # < 2B\n",
    "        }\n",
    "        \n",
    "        for node_id, attrs in self.graph.nodes(data=True):\n",
    "            if attrs.get('type') == 'company':\n",
    "                market_cap = attrs.get('mkt_market_cap', 0)\n",
    "                \n",
    "                if market_cap > 200e9:\n",
    "                    cap_ranges['mega_cap'].append(node_id)\n",
    "                elif market_cap > 10e9:\n",
    "                    cap_ranges['large_cap'].append(node_id)\n",
    "                elif market_cap > 2e9:\n",
    "                    cap_ranges['mid_cap'].append(node_id)\n",
    "                else:\n",
    "                    cap_ranges['small_cap'].append(node_id)\n",
    "        \n",
    "        # Create connections within each market cap range\n",
    "        edge_count = 0\n",
    "        for cap_range, companies in cap_ranges.items():\n",
    "            if len(companies) > 1:\n",
    "                # Connect each company to a few others in the same cap range\n",
    "                import random\n",
    "                for company in companies:\n",
    "                    peers = [c for c in companies if c != company]\n",
    "                    selected_peers = random.sample(peers, min(5, len(peers)))\n",
    "                    \n",
    "                    for peer in selected_peers:\n",
    "                        if not self.graph.has_edge(company, peer):\n",
    "                            self.graph.add_edge(company, peer, relationship='similar_market_cap')\n",
    "                            edge_count += 1\n",
    "        \n",
    "        logger.info(f\"Added {edge_count} market cap similarity edges\")\n",
    "\n",
    "        \n",
    "    def _extract_financial_metrics(self, facts):\n",
    "        \"\"\"Extract key financial metrics from SEC facts\"\"\"\n",
    "        \n",
    "        financial_attrs = {}\n",
    "        \n",
    "        # Enhanced GAAP metrics\n",
    "        gaap_metrics = {\n",
    "            'Assets': 'total_assets',\n",
    "            'Revenues': 'revenues',\n",
    "            'NetIncomeLoss': 'net_income',\n",
    "            'StockholdersEquity': 'stockholders_equity',\n",
    "            'CashAndCashEquivalentsAtCarryingValue': 'cash',\n",
    "            'LongTermDebt': 'long_term_debt'\n",
    "        }\n",
    "        \n",
    "        us_gaap = facts.get('us-gaap', {})\n",
    "        \n",
    "        for gaap_key, attr_name in gaap_metrics.items():\n",
    "            if gaap_key in us_gaap:\n",
    "                metric_data = us_gaap[gaap_key]\n",
    "                if 'units' in metric_data and 'USD' in metric_data['units']:\n",
    "                    usd_data = metric_data['units']['USD']\n",
    "                    if usd_data:\n",
    "                        # Get most recent annual value\n",
    "                        latest_value = max(usd_data, key=lambda x: x.get('end', ''))\n",
    "                        financial_attrs[attr_name] = latest_value.get('val', 0)\n",
    "        \n",
    "        return financial_attrs\n",
    "    \n",
    "    def _process_market_attributes(self, market_data):\n",
    "        \"\"\"Process and normalize market data attributes\"\"\"\n",
    "        \n",
    "        market_attrs = {}\n",
    "        \n",
    "        # Direct market attributes\n",
    "        market_fields = [\n",
    "            'current_price', 'market_cap', 'pe_ratio', 'pb_ratio',\n",
    "            'dividend_yield', 'beta', 'profit_margins',\n",
    "            'return_on_equity', 'debt_to_equity', 'revenue_growth',\n",
    "            'year_return'\n",
    "        ]\n",
    "        \n",
    "        for field in market_fields:\n",
    "            value = market_data.get(field, 0)\n",
    "            # Handle None values and convert to float\n",
    "            if value is not None and value != '':\n",
    "                try:\n",
    "                    market_attrs[f\"mkt_{field}\"] = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    market_attrs[f\"mkt_{field}\"] = 0.0\n",
    "            else:\n",
    "                market_attrs[f\"mkt_{field}\"] = 0.0\n",
    "        \n",
    "        return market_attrs\n",
    "    \n",
    "    def _add_market_relationships(self, company):\n",
    "        \"\"\"Add market data relationships and nodes\"\"\"\n",
    "        \n",
    "        cik = str(company['cik'])\n",
    "        market_data = company.get('market_data', {})\n",
    "        tickers = company.get('tickers', [])\n",
    "        \n",
    "        if not tickers or not market_data:\n",
    "            return\n",
    "            \n",
    "        ticker = tickers[0]\n",
    "        \n",
    "        # Add price node\n",
    "        current_price = market_data.get('current_price', 0)\n",
    "        if current_price and current_price > 0:\n",
    "            price_node = f\"price_{ticker}\"\n",
    "            if not self.graph.has_node(price_node):\n",
    "                self.graph.add_node(price_node, \n",
    "                                  type='market_metric',\n",
    "                                  metric='current_price',\n",
    "                                  value=current_price,\n",
    "                                  ticker=ticker)\n",
    "            self.graph.add_edge(cik, price_node, relationship='has_price')\n",
    "            self.relationship_types.add('has_price')\n",
    "    \n",
    "    def _add_financial_relationships(self, company):\n",
    "        \"\"\"Add financial relationships based on metrics\"\"\"\n",
    "        \n",
    "        cik = str(company['cik'])\n",
    "        facts = company.get('facts', {})\n",
    "        \n",
    "        # Add metric nodes and relationships\n",
    "        us_gaap = facts.get('us-gaap', {})\n",
    "        for metric_name, metric_data in list(us_gaap.items())[:5]:  # Limit to avoid too many nodes\n",
    "            if 'units' in metric_data:\n",
    "                metric_node = f\"metric_{metric_name}\"\n",
    "                \n",
    "                if not self.graph.has_node(metric_node):\n",
    "                    self.graph.add_node(metric_node, type='metric', name=metric_name)\n",
    "                \n",
    "                self.graph.add_edge(cik, metric_node, relationship='reports')\n",
    "                self.relationship_types.add('reports')\n",
    "                \n",
    "    def _add_industry_relationships(self, company):\n",
    "        \"\"\"Add industry-based relationships\"\"\"\n",
    "        \n",
    "        cik = str(company['cik'])\n",
    "        sic = company.get('sic', '')\n",
    "        sic_description = company.get('sicDescription', '')\n",
    "        \n",
    "        if sic and sic_description:\n",
    "            industry_node = f\"industry_{sic}\"\n",
    "            \n",
    "            if not self.graph.has_node(industry_node):\n",
    "                self.graph.add_node(industry_node, \n",
    "                                  type='industry', \n",
    "                                  sic=sic, \n",
    "                                  description=sic_description)\n",
    "            \n",
    "            self.graph.add_edge(cik, industry_node, relationship='belongs_to')\n",
    "            self.relationship_types.add('belongs_to')\n",
    "    \n",
    "    def get_enhanced_node_features(self, node_id):\n",
    "        \"\"\"\n",
    "        Extracts enhanced numerical features for a node in a consistent, predefined order.\n",
    "        \"\"\"\n",
    "        # 1. Handle nodes that might not be in the graph (e.g., during edge cases)\n",
    "        if node_id not in self.graph.nodes:\n",
    "            return np.zeros(len(self.feature_names), dtype=np.float32)\n",
    "    \n",
    "        attrs = self.graph.nodes[node_id]\n",
    "        features_dict = {}  # Build features in a dictionary for robustness\n",
    "    \n",
    "        # 2. Type encoding (one-hot)\n",
    "        node_type = attrs.get('type', 'unknown')\n",
    "        features_dict['type_company'] = 1.0 if node_type == 'company' else 0.0\n",
    "        features_dict['type_metric'] = 1.0 if node_type == 'metric' else 0.0\n",
    "        features_dict['type_industry'] = 1.0 if node_type == 'industry' else 0.0\n",
    "        features_dict['type_market_metric'] = 1.0 if node_type == 'market_metric' else 0.0\n",
    "    \n",
    "        # 3. SEC financial metrics (log-normalized to handle large value ranges)\n",
    "        sec_metrics = ['total_assets', 'revenues', 'net_income', 'stockholders_equity']\n",
    "        for metric in sec_metrics:\n",
    "            value = float(attrs.get(metric, 0.0))\n",
    "            features_dict[metric] = np.log1p(max(0, value))\n",
    "    \n",
    "        # 4. Market data features (with specific normalization)\n",
    "        market_metrics = [\n",
    "            'mkt_current_price', 'mkt_market_cap', 'mkt_pe_ratio', 'mkt_pb_ratio',\n",
    "            'mkt_dividend_yield', 'mkt_beta', 'mkt_return_on_equity', 'mkt_debt_to_equity'\n",
    "        ]\n",
    "        for metric in market_metrics:\n",
    "            value = float(attrs.get(metric, 0.0))\n",
    "            if 'ratio' in metric or 'yield' in metric:\n",
    "                normalized_value = max(-1.0, min(1.0, value))\n",
    "            elif 'price' in metric or 'cap' in metric:\n",
    "                normalized_value = np.log1p(max(0, value))\n",
    "            else: # Covers beta and other metrics\n",
    "                normalized_value = max(-3.0, min(3.0, value))\n",
    "            features_dict[metric] = normalized_value\n",
    "            \n",
    "        # 5. SIC code encoding (normalized)\n",
    "        # Handle cases where 'sic' can be None or an integer. The original code\n",
    "        # failed with an AttributeError if attrs.get('sic') returned None.\n",
    "        sic_raw = attrs.get('sic') \n",
    "        # Safely convert the raw value to a string. None becomes an empty string ''.\n",
    "        sic = str(sic_raw) if sic_raw is not None else ''\n",
    "        \n",
    "        # Now that `sic` is guaranteed to be a string, this check is safe.\n",
    "        sic_numeric = float(sic) if sic.isdigit() else 0.0\n",
    "        features_dict['sic_numeric'] = sic_numeric / 10000.0\n",
    "    \n",
    "        # 6. Categorical counts\n",
    "        features_dict['num_exchanges'] = float(len(attrs.get('exchanges', [])))\n",
    "        features_dict['num_tickers'] = float(len(attrs.get('tickers', [])))\n",
    "    \n",
    "        # 7. Assemble the final feature vector in the correct order\n",
    "        # This is the most important step for robustness. It uses the predefined\n",
    "        # list to order the features and provides a default of 0.0 if a key is missing.\n",
    "        final_features = [features_dict.get(name, 0.0) for name in self.feature_names]\n",
    "        \n",
    "        return np.array(final_features, dtype=np.float32)\n",
    "\n",
    "# \n",
    "# HYBRID LLM-GNN INTEGRATION\n",
    "# \n",
    "\n",
    "# class HybridLLMGNNFinancialSystem(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Advanced hybrid system integrating LLM and GNN with bidirectional information flow\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, config, llm_agent: FineTunedPhi4Agent, e5_embedder: MultilingualE5Embedder):\n",
    "#         super().__init__()\n",
    "            \n",
    "#         self.config = config\n",
    "        \n",
    "#         # Initialize E5 embedder (as in your existing system)\n",
    "#         self.e5_embedder = e5_embedder\n",
    "        \n",
    "#         # GNN component\n",
    "#         self.gnn = AttentionBasedFinancialGNN(\n",
    "#             input_dim=1024,  # E5 embedding dimension\n",
    "#             hidden_dim=config.embedding_dim,  # Use your config setting\n",
    "#             num_heads=8,\n",
    "#             num_layers=3\n",
    "#         )\n",
    "        \n",
    "#         # LLM component (your existing Phi-4)\n",
    "#         self.llm_agent = llm_agent\n",
    "        \n",
    "#         # Cross-modal fusion layers\n",
    "#         self.llm_to_gnn_adapter = nn.Sequential(\n",
    "#             nn.Linear(1024, config.embedding_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.LayerNorm(config.embedding_dim)\n",
    "#         )\n",
    "        \n",
    "#         gnn_output_dim = config.embedding_dim // 8  # 1024 / 8 = 128\n",
    "        \n",
    "#         self.gnn_to_llm_adapter = nn.Sequential(\n",
    "#             nn.Linear(gnn_output_dim, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.LayerNorm(1024)\n",
    "#         )\n",
    "        \n",
    "#         # Bidirectional attention mechanism\n",
    "#         self.cross_modal_attention = nn.MultiheadAttention(\n",
    "#             embed_dim=1024,\n",
    "#             num_heads=8,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "    \n",
    "#     def unified_prediction(self, text_data, graph_data, task='risk_assessment'):\n",
    "#         \"\"\"\n",
    "#         Unified prediction combining LLM and GNN insights\n",
    "#         \"\"\"\n",
    "#         # Monitor memory before heavy operations\n",
    "#         if torch.cuda.is_available():\n",
    "#             allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "#             reserved = torch.cuda.memory_reserved() / 1024**3   # GB\n",
    "#             logger.info(f\"Memory before prediction - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "        \n",
    "#         # Stage 1: GNN analysis\n",
    "#         x, edge_index = graph_data['x'], graph_data['edge_index']\n",
    "#         gnn_output = self.gnn(x, edge_index)\n",
    "#         graph_embeddings = gnn_output['node_embeddings']\n",
    "        \n",
    "#         # Stage 2: LLM analysis\n",
    "#         text_embeddings = self.e5_embedder.encode_passages(text_data)\n",
    "#         text_embeddings = torch.tensor(text_embeddings).to(graph_embeddings.device)\n",
    "        \n",
    "#         # Stage 3: Cross-modal fusion\n",
    "#         adapted_graph_features = self.gnn_to_llm_adapter(graph_embeddings)\n",
    "        \n",
    "#         # Enhance text embeddings with graph structure\n",
    "#         enhanced_text, attention_weights = self.cross_modal_attention(\n",
    "#             text_embeddings.unsqueeze(0),\n",
    "#             adapted_graph_features.unsqueeze(0),\n",
    "#             adapted_graph_features.unsqueeze(0)\n",
    "#         )\n",
    "        \n",
    "#         # Generate explanation\n",
    "#         explanation = self._generate_interpretation(gnn_output, enhanced_text, task)\n",
    "        \n",
    "#         return {\n",
    "#             'gnn_output': gnn_output,\n",
    "#             'enhanced_text': enhanced_text.squeeze(0),\n",
    "#             'attention_weights': attention_weights,\n",
    "#             'explanation': explanation\n",
    "#         }\n",
    "    \n",
    "#     def _generate_interpretation(self, gnn_output, enhanced_text, task):\n",
    "#         \"\"\"Generate human-readable interpretation\"\"\"\n",
    "        \n",
    "#         interpretation_prompt = f\"\"\"\n",
    "#         Based on the hybrid LLM-GNN analysis for {task}, provide an explanation:\n",
    "        \n",
    "#         Key factors influencing this {task} decision based on graph neural network \n",
    "#         attention patterns and enhanced language understanding.\n",
    "        \n",
    "#         Explain the reasoning in simple terms.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         explanation = self.llm_agent.generate(interpretation_prompt, max_new_tokens=150)\n",
    "        \n",
    "#         return {\n",
    "#             'explanation': explanation,\n",
    "#             'confidence_factors': {\n",
    "#                 'gnn_attention_strength': float(torch.mean(torch.stack([\n",
    "#                     # GATv2Conv returns a tuple (edge_index, scores). We only want the scores [1].\n",
    "#                     w[1].mean() for w in gnn_output.get('attention_weights', []) \n",
    "#                     if w is not None and isinstance(w, tuple) and len(w) > 1\n",
    "#                 ]) if gnn_output.get('attention_weights') else torch.tensor([0.5]))),\n",
    "#                 'risk_assessment': float(gnn_output['risk_scores'].mean()) if 'risk_scores' in gnn_output else 0.5\n",
    "#             }\n",
    "#         }\n",
    "#     # Clear cache after prediction\n",
    "#     if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "# \n",
    "# QDRANT STORAGE SYSTEM\n",
    "# \n",
    "\n",
    "class QdrantEmbeddingStorage:\n",
    "    \"\"\"\n",
    "    MODIFIED: A pure inference client for Qdrant that loads from and searches in\n",
    "    multiple, model-specific collections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, e5_embedder: \"MultilingualE5Embedder\", storage_path=\"./financial_embeddings_db\"):\n",
    "        self.storage_path = storage_path\n",
    "        self.embedder = e5_embedder\n",
    "        try:\n",
    "            self.client = QdrantClient(path=self.storage_path)\n",
    "            logger.info(f\"Connected to Qdrant local mode at {self.storage_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Qdrant client: {e}\", exc_info=True)\n",
    "            self.client = QdrantClient(\":memory:\")\n",
    "            logger.warning(\"Falling back to in-memory Qdrant client.\")\n",
    "\n",
    "    def load_collection(self, collection_name: str) -> Tuple[Dict[str, np.ndarray], Dict[str, Dict]]:\n",
    "        \"\"\"Loads all vectors and payloads from a single specified collection.\"\"\"\n",
    "        embeddings_dict = {}\n",
    "        node_attributes = {}\n",
    "        \n",
    "        try:\n",
    "            self.client.get_collection(collection_name=collection_name)\n",
    "            scroll_result, _ = self.client.scroll(\n",
    "                collection_name=collection_name, limit=20000,\n",
    "                with_payload=True, with_vectors=True\n",
    "            )\n",
    "            \n",
    "            for point in scroll_result:\n",
    "                node_id = point.payload.get(\"node_id\", str(point.id))\n",
    "                if node_id:\n",
    "                    embeddings_dict[node_id] = np.array(point.vector)\n",
    "                    node_attributes[node_id] = point.payload\n",
    "                    \n",
    "            logger.info(f\"Loaded {len(embeddings_dict)} points from collection '{collection_name}'\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Collection '{collection_name}' not found or is empty.\") from e\n",
    "\n",
    "        return embeddings_dict, node_attributes\n",
    "        \n",
    "    def search_similar_entities(self, model_type: str, query_text: Optional[str] = None, \n",
    "                                query_embedding: Optional[np.ndarray] = None, \n",
    "                                top_k: int = 10, filters: Optional[dict] = None) -> List[dict]:\n",
    "        \"\"\"\n",
    "        MODIFIED: Searches in a model-specific collection and DEFAULTS to filtering for companies.\n",
    "        \"\"\"\n",
    "        collection_name = f\"financial_entities_{model_type}\"\n",
    "    \n",
    "        if query_embedding is None and query_text is not None:\n",
    "            query_vector = self.embedder.encode_queries([query_text])[0]\n",
    "        elif query_embedding is not None:\n",
    "            query_vector = query_embedding\n",
    "        else:\n",
    "            raise ValueError(\"Either query_text or query_embedding must be provided.\")\n",
    "    \n",
    "        try:\n",
    "            # Default to a company-only filter if none is provided.\n",
    "            if filters is None:\n",
    "                filters = {\"type\": \"company\"}\n",
    "    \n",
    "            qdrant_filter = self._convert_filters(filters)\n",
    "\n",
    "            # Dynamically determine the collection to search\n",
    "            collection_name = f\"financial_entities_{model_type}\"\n",
    "\n",
    "            # Use the modern .search() method with correct parameters\n",
    "            search_results = self.client.search(\n",
    "                collection_name = collection_name,\n",
    "                query_vector=query_vector.tolist(),\n",
    "                query_filter=qdrant_filter,\n",
    "                limit=top_k,\n",
    "                with_payload=True\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            similar_entities = []\n",
    "            for result in search_results: # Iterating directly over the result list\n",
    "                payload = result.payload\n",
    "                similar_entities.append({\n",
    "                    \"entity_id\": str(result.id),\n",
    "                    \"name\": payload.get(\"name\", str(result.id)),\n",
    "                    \"type\": payload.get(\"type\", \"unknown\"),\n",
    "                    \"ticker\": payload.get(\"ticker\", \"\"),\n",
    "                    \"similarity_score\": float(result.score),\n",
    "                    \"market_cap\": payload.get(\"market_cap\", 0),\n",
    "                    \"pe_ratio\": payload.get(\"pe_ratio\", 0),\n",
    "                    \"sector\": payload.get(\"sector\", \"\"),\n",
    "                    \"metadata\": payload\n",
    "                })\n",
    "            \n",
    "            return similar_entities\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching similar entities in Qdrant: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "\n",
    "    def filter_by_payload(self, model_type: str, filters: dict, limit: int = 1) -> List[dict]:\n",
    "        \"\"\"Retrieves points based on an exact payload filter, using scroll.\"\"\"\n",
    "        collection_name = f\"financial_entities_{model_type}\"\n",
    "        try:\n",
    "            qdrant_filter = self._convert_filters(filters)\n",
    "    \n",
    "            # Use scroll for exact filtering instead of search\n",
    "            scroll_result, _ = self.client.scroll(\n",
    "                collection_name=collection_name,\n",
    "                scroll_filter=qdrant_filter,\n",
    "                limit=limit,\n",
    "                with_payload=True\n",
    "            )\n",
    "    \n",
    "            similar_entities = []\n",
    "            for result in scroll_result:\n",
    "                payload = result.payload or {}\n",
    "                similar_entities.append({\n",
    "                    \"id\": result.id,\n",
    "                    \"name\": payload.get(\"name\", \"Unknown Entity\"),\n",
    "                    \"type\": payload.get(\"type\", \"unknown\"),\n",
    "                    \"ticker\": payload.get(\"ticker\", \"\"),\n",
    "                    \"similarity_score\": 1.0,  # It's an exact match\n",
    "                    \"metadata\": payload\n",
    "                })\n",
    "            return similar_entities\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Scroll-based filtering failed in collection '{collection_name}': {e}\")\n",
    "            return []\n",
    "\n",
    "    \n",
    "    def _convert_filters(self, filters: Optional[dict]) -> Optional[Filter]:\n",
    "        \"\"\" filter conversion that handles $and, $or operators\"\"\"\n",
    "        if not filters:\n",
    "            return None\n",
    "        \n",
    "        conditions = []\n",
    "        \n",
    "        # Handle boolean operators\n",
    "        if \"$and\" in filters:\n",
    "            and_conditions = []\n",
    "            for condition in filters[\"$and\"]:\n",
    "                and_conditions.extend(self._parse_filter_condition(condition))\n",
    "            return Filter(must=and_conditions)\n",
    "        \n",
    "        elif \"$or\" in filters:\n",
    "            or_conditions = []\n",
    "            for condition in filters[\"$or\"]:\n",
    "                or_conditions.extend(self._parse_filter_condition(condition))\n",
    "            return Filter(should=or_conditions)\n",
    "        \n",
    "        else:\n",
    "            # Simple filters\n",
    "            conditions = self._parse_filter_condition(filters)\n",
    "            return Filter(must=conditions) if conditions else None\n",
    "    \n",
    "    def _parse_filter_condition(self, condition: dict) -> List:\n",
    "        \"\"\"Parse individual filter conditions\"\"\"\n",
    "        conditions = []\n",
    "        \n",
    "        for key, value in condition.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Range conditions like {\"$gt\": 10}\n",
    "                range_conditions = {}\n",
    "                for op, val in value.items():\n",
    "                    clean_op = op.replace('$', '')\n",
    "                    range_conditions[clean_op] = val\n",
    "                conditions.append(FieldCondition(key=key, range=Range(**range_conditions)))\n",
    "            else:\n",
    "                # Exact match\n",
    "                conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))\n",
    "        \n",
    "        return conditions\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "# EXPLAINABLE AI FOR FINANCIAL DECISIONS\n",
    "# \n",
    "\n",
    "# class FinancialExplainerAgent:\n",
    "#     \"\"\"\n",
    "#     Explainable AI agent for financial GNN decisions\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, config, llm_agent: FineTunedPhi4Agent):\n",
    "#         self.config = config\n",
    "#         self.llm_agent = llm_agent\n",
    "    \n",
    "#     def explain_prediction(self, model_output, query, entity_data):\n",
    "#         \"\"\"\n",
    "#         Generate comprehensive explanation for financial prediction\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Extract key information from model output\n",
    "#         attention_weights = model_output.get('attention_weights', [])\n",
    "#         risk_scores = model_output.get('risk_scores', torch.tensor([0.5]))\n",
    "#         node_embeddings = model_output.get('node_embeddings', torch.tensor([]))\n",
    "        \n",
    "#         # Calculate importance scores\n",
    "#         importance_analysis = self._analyze_importance(attention_weights, entity_data)\n",
    "        \n",
    "#         # Generate narrative explanation\n",
    "#         narrative = self._generate_narrative_explanation(\n",
    "#             query, importance_analysis, risk_scores.mean().item()\n",
    "#         )\n",
    "        \n",
    "#         # Create evidence trail\n",
    "#         evidence_trail = self._create_evidence_trail(importance_analysis)\n",
    "        \n",
    "#         return {\n",
    "#             'prediction_explanation': narrative,\n",
    "#             'evidence_trail': evidence_trail,\n",
    "#             'confidence_score': self._calculate_confidence(importance_analysis),\n",
    "#             'key_factors': importance_analysis['top_factors']\n",
    "#         }\n",
    "    \n",
    "#     def _analyze_importance(self, attention_weights, entity_data):\n",
    "#         \"\"\"Analyze attention weights to determine important factors\"\"\"\n",
    "        \n",
    "#         if not attention_weights or not entity_data:\n",
    "#             return {'top_factors': [], 'attention_distribution': []}\n",
    "        \n",
    "#         # Simple importance analysis\n",
    "#         top_factors = []\n",
    "        \n",
    "#         # Add entity-based factors\n",
    "#         for entity in entity_data[:3]:  # Top 3 entities\n",
    "#             factor = {\n",
    "#                 'factor_type': 'entity_similarity',\n",
    "#                 'entity_name': entity.get('name', 'Unknown'),\n",
    "#                 'importance_score': entity.get('similarity_score', 0.5),\n",
    "#                 'description': f\"Similar to {entity.get('name', 'Unknown')} in {entity.get('sector', 'unknown sector')}\"\n",
    "#             }\n",
    "#             top_factors.append(factor)\n",
    "        \n",
    "#         return {\n",
    "#             'top_factors': top_factors,\n",
    "#             'attention_distribution': [0.8, 0.6, 0.4]  # Placeholder\n",
    "#         }\n",
    "    \n",
    "#     def _generate_narrative_explanation(self, query, importance_analysis, risk_score):\n",
    "#         \"\"\"Generate human-readable narrative explanation\"\"\"\n",
    "        \n",
    "#         factors_text = \"\\n\".join([\n",
    "#             f\"- {factor['description']} (importance: {factor['importance_score']:.2f})\"\n",
    "#             for factor in importance_analysis['top_factors']\n",
    "#         ])\n",
    "        \n",
    "#         explanation_prompt = f\"\"\"\n",
    "#         Explain this financial analysis result in simple terms:\n",
    "        \n",
    "#         Query: {query}\n",
    "#         Risk Score: {risk_score:.3f}\n",
    "        \n",
    "#         Key Contributing Factors:\n",
    "#         {factors_text}\n",
    "        \n",
    "#         Provide a clear, professional explanation of why this assessment was made.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         explanation = self.llm_agent.generate(\n",
    "#             explanation_prompt,\n",
    "#             max_new_tokens=200,\n",
    "#             temperature=0.2\n",
    "#         )\n",
    "        \n",
    "#         return explanation.strip()\n",
    "    \n",
    "#     def _create_evidence_trail(self, importance_analysis):\n",
    "#         \"\"\"Create detailed evidence trail for audit purposes\"\"\"\n",
    "        \n",
    "#         evidence_items = []\n",
    "        \n",
    "#         for factor in importance_analysis['top_factors']:\n",
    "#             evidence_items.append({\n",
    "#                 'evidence_type': factor['factor_type'],\n",
    "#                 'description': factor['description'],\n",
    "#                 'confidence': factor['importance_score'],\n",
    "#                 'impact_level': 'high' if factor['importance_score'] > 0.7 else 'medium' if factor['importance_score'] > 0.4 else 'low'\n",
    "#             })\n",
    "        \n",
    "#         return evidence_items\n",
    "    \n",
    "#     def _calculate_confidence(self, importance_analysis):\n",
    "#         \"\"\"Calculate overall confidence in the explanation\"\"\"\n",
    "        \n",
    "#         if not importance_analysis['top_factors']:\n",
    "#             return 0.5\n",
    "        \n",
    "#         avg_importance = sum(\n",
    "#             factor['importance_score'] \n",
    "#             for factor in importance_analysis['top_factors']\n",
    "#         ) / len(importance_analysis['top_factors'])\n",
    "        \n",
    "#         return min(avg_importance, 1.0)\n",
    "\n",
    "# \n",
    "# NUMERICAL REASONING AGENT\n",
    "# \n",
    "class EnhancedNumericalReasoning:\n",
    "    \"\"\"\n",
    "    An agent that uses a two-stage process to solve numerical queries,\n",
    "    separating reasoning from calculation to improve accuracy.\n",
    "    \"\"\"\n",
    "    def __init__(self, phi4_agent: FineTunedPhi4Agent):\n",
    "        self.phi4_agent = phi4_agent\n",
    "\n",
    "    def numerical_query_with_steps(self, query: str) -> dict:\n",
    "        \"\"\"Two-stage numerical reasoning with intermediate steps.\"\"\"\n",
    "        \n",
    "        # Stage 1: Generate a plan with reasoning and calculation steps.\n",
    "        reasoning_prompt = f\"\"\"\n",
    "        Break down the following financial query into reasoning steps and a list of precise mathematical operations.\n",
    "        Query: {query}\n",
    "\n",
    "        Provide your response in a pure JSON format with no extra text:\n",
    "        {{\n",
    "            \"steps\": [\"step 1 describing the logic\", \"step 2 describing the next part of the logic\"],\n",
    "            \"calculations\": [\"1280 / 1366\", \"(result) * 100\"],\n",
    "            \"reasoning\": \"A brief explanation of the financial formula being used.\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        stage1_response = self.phi4_agent.generate(reasoning_prompt, max_new_tokens=200, temperature=0.2)\n",
    "        intermediate_steps = extract_json_from_response(stage1_response)\n",
    "\n",
    "        if not intermediate_steps or 'calculations' not in intermediate_steps:\n",
    "            return {\"error\": \"Failed to generate calculation steps in Stage 1.\"}\n",
    "\n",
    "        # Stage 2: Execute the calculations and provide the final answer.\n",
    "        final_prompt = f\"\"\"\n",
    "        Given the following list of calculations: {intermediate_steps.get('calculations', [])}\n",
    "        \n",
    "        Execute each calculation precisely and provide the final numerical answer in a pure JSON format:\n",
    "        {{\n",
    "            \"final_calculations\": [\"1280 / 1366 = 0.937\", \"0.937 * 100 = 93.7\"],\n",
    "            \"answer\": \"93.7%\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        final_response = self.phi4_agent.generate(final_prompt, max_new_tokens=150, temperature=0.2)\n",
    "        \n",
    "        return {\n",
    "            'intermediate_steps': intermediate_steps,\n",
    "            'final_result': extract_json_from_response(final_response)\n",
    "        }\n",
    "# \n",
    "# UNIFIED KNOWLEDGE GRAPH AGENT (COMBINING BOTH APPROACHES)\n",
    "# \n",
    "\n",
    "class UnifiedKnowledgeGraphAgent:\n",
    "    \"\"\"Inference-only agent that loads pre-trained embeddings from multiple model collections\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SystemConfig, e5_embedder: MultilingualE5Embedder):\n",
    "        self.config = config\n",
    "        self.storage_path = config.graph_storage_path\n",
    "        self.embedding_model = e5_embedder\n",
    "        \n",
    "        # Initialize storage client\n",
    "        self.storage = QdrantEmbeddingStorage(\n",
    "            e5_embedder=self.embedding_model,\n",
    "            storage_path=self.storage_path\n",
    "        )\n",
    "        \n",
    "        # Load all available embeddings on initialization\n",
    "        self.embeddings_by_model, self.node_attributes = self._load_all_embeddings()\n",
    "        \n",
    "        # Performance cache\n",
    "        self.similarity_cache = LRUCache(maxsize=1000)\n",
    "        \n",
    "        logger.info(f\" Loaded {sum(len(v) for v in self.embeddings_by_model.values())} embeddings \"\n",
    "                   f\"from {len(self.embeddings_by_model)} model types\")\n",
    "     \n",
    "    def resolve_entity(self, entity_query: str) -> List[dict]:\n",
    "        \"\"\"Resolves an entity using a multi-step waterfall approach before falling back.\"\"\"\n",
    "        \n",
    "        #  Waterfall Step 1: Exact, Case-Sensitive Match \n",
    "        # The fastest and most accurate check.\n",
    "        logger.info(f\"Attempting exact match for '{entity_query}'...\")\n",
    "        exact_match_filter = {\"name\": entity_query}\n",
    "        results = self.storage.filter_by_payload('e5', filters=exact_match_filter)\n",
    "        if results:\n",
    "            return results\n",
    "    \n",
    "        #  Waterfall Step 2: Case-Insensitive Match \n",
    "        # Handles variations like \"apple\" vs \"Apple\".\n",
    "        logger.info(f\"Attempting case-insensitive match for '{entity_query.title()}'...\")\n",
    "        caseless_match_filter = {\"name\": entity_query.title()}\n",
    "        results = self.storage.filter_by_payload('e5', filters=caseless_match_filter)\n",
    "        if results:\n",
    "            return results\n",
    "    \n",
    "        \n",
    "        #  Final Step: Semantic Search Fallback \n",
    "        #logger.info(f\"No exact match found. Falling back to semantic search for '{entity_query}'...\")\n",
    "        return self.find_similar_entities(entity_query, model_type='e5', top_k=1)\n",
    "        \n",
    "    def _load_all_embeddings(self):\n",
    "        \"\"\"Load embeddings from all trained model collections\"\"\"\n",
    "        embeddings_by_model = {}\n",
    "        all_node_attributes = {}\n",
    "        \n",
    "        # Expected model types from your training pipeline\n",
    "        model_types = ['graphsage', 'attention_gnn', 'temporal_gnn', 'e5']\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            try:\n",
    "                # Search in the model-specific collection\n",
    "                collection_name = f\"financial_entities_{model_type}\"\n",
    "                embeddings, attributes = self._load_collection(collection_name)\n",
    "                \n",
    "                if embeddings:\n",
    "                    embeddings_by_model[model_type] = embeddings\n",
    "                    all_node_attributes.update(attributes)\n",
    "                    logger.info(f\" Loaded {len(embeddings)} {model_type} embeddings\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"⚠️ Could not load {model_type} embeddings: {e}\")\n",
    "        \n",
    "        return embeddings_by_model, all_node_attributes\n",
    "    \n",
    "    def find_relationship_path(self, source_entity: str, target_entity: str) -> Optional[dict]:\n",
    "        \"\"\"Safe stub for relationship path finding\"\"\"\n",
    "        try:\n",
    "            # Find both entities\n",
    "            source_results = self.find_similar_entities(source_entity, top_k=1)\n",
    "            target_results = self.find_similar_entities(target_entity, top_k=1)\n",
    "            \n",
    "            if source_results and target_results:\n",
    "                # Simple similarity-based relationship\n",
    "                source_sector = source_results[0].get('metadata', {}).get('sector', '')\n",
    "                target_sector = target_results.get('metadata', {}).get('sector', '')\n",
    "                \n",
    "                if source_sector == target_sector and source_sector:\n",
    "                    return {\n",
    "                        \"type\": \"direct\",\n",
    "                        \"path\": [source_entity, target_entity],\n",
    "                        \"confidence\": 0.8,\n",
    "                        \"relationships\": [\"same_sector\"]\n",
    "                    }\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Relationship path finding failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def _load_collection(self, collection_name):\n",
    "        \"\"\"\n",
    "        Load ALL embeddings from a Qdrant collection by handling pagination.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            embeddings_dict = {}\n",
    "            node_attributes = {}\n",
    "            next_page_offset = None  # Start with no offset\n",
    "\n",
    "            # Loop until there are no more pages of results\n",
    "            while True:\n",
    "                # Fetch a batch of points\n",
    "                scroll_result, next_page_offset = self.storage.client.scroll(\n",
    "                    collection_name=collection_name,\n",
    "                    limit=1000,  # A smaller batch size is more memory-efficient\n",
    "                    offset=next_page_offset,\n",
    "                    with_payload=True,\n",
    "                    with_vectors=True\n",
    "                )\n",
    "\n",
    "                # Process the points from the current batch\n",
    "                for point in scroll_result:\n",
    "                    node_id = point.payload.get(\"node_id\", str(point.id))\n",
    "                    embeddings_dict[node_id] = np.array(point.vector)\n",
    "                    node_attributes[node_id] = point.payload\n",
    "                \n",
    "                # If there's no next_page_offset, we've reached the end\n",
    "                if not next_page_offset:\n",
    "                    break\n",
    "            \n",
    "            total_loaded = len(embeddings_dict)\n",
    "            if total_loaded > 0:\n",
    "                logger.info(f\" Loaded a total of {total_loaded} points from collection '{collection_name}'\")\n",
    "            \n",
    "            return embeddings_dict, node_attributes\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load collection {collection_name}: {e}\")\n",
    "            return {}, {}\n",
    "    \n",
    "    def find_similar_entities(self, entity_name: str, model_type: str = 'e5', \n",
    "                             top_k: int = 10, filters: dict = None):\n",
    "        # Now returns entities enriched with SEC EDGAR financial data\n",
    "        return self.storage.search_similar_entities(\n",
    "            model_type=model_type,\n",
    "            query_text=entity_name,\n",
    "            top_k=top_k,\n",
    "            filters=filters\n",
    "        )\n",
    "    \n",
    "    def get_available_model_types(self):\n",
    "        \"\"\"Return list of available embedding model types\"\"\"\n",
    "        return list(self.embeddings_by_model.keys())\n",
    "    \n",
    "    def get_peer_companies(self, entity_name: str, model_type: str = 'graphsage', \n",
    "                          top_k: int = 5):\n",
    "        \"\"\"Get peer companies using graph-based embeddings\"\"\"\n",
    "        \n",
    "        # Find the entity first\n",
    "        entity_results = self.find_similar_entities(entity_name, model_type, top_k=1)\n",
    "        if not entity_results:\n",
    "            return []\n",
    "        \n",
    "        # Get sector from the entity\n",
    "        entity_data = entity_results[0].get('metadata', {})\n",
    "        sector = entity_data.get('sector', '')\n",
    "        \n",
    "        if sector:\n",
    "            # Find companies in same sector\n",
    "            sector_filter = {\"type\": \"company\", \"sector\": sector}\n",
    "            peers = self.find_similar_entities(\n",
    "                entity_name=sector,\n",
    "                model_type=model_type,\n",
    "                top_k=top_k + 1,\n",
    "                filters=sector_filter\n",
    "            )\n",
    "            # Remove original entity\n",
    "            return [p for p in peers if p.get('name') != entity_results[0].get('name')]\n",
    "        \n",
    "        return []\n",
    "\n",
    "# \n",
    "# ENHANCED RETRIEVAL SYSTEM\n",
    "# \n",
    "\n",
    "class EnhancedRetrieverAgent:\n",
    "    \"\"\"Enhanced retriever with hybrid text + graph embedding search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SystemConfig, kg_agent: UnifiedKnowledgeGraphAgent, e5_embedder: MultilingualE5Embedder):\n",
    "        self.config = config\n",
    "        self.kg_agent = kg_agent\n",
    "        self.embedding_model = e5_embedder\n",
    "        self.vector_index = faiss.IndexFlatL2(config.vector_db_dimension)\n",
    "        self.document_metadata = []\n",
    "        \n",
    "        # Weight factors for hybrid search\n",
    "        self.text_weight = 0.6\n",
    "        self.graph_weight = 0.4\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Hybrid search combining document and graph embeddings\"\"\"\n",
    "        \n",
    "        # Text-based document search (placeholder - would integrate with actual document store)\n",
    "        text_results = self._placeholder_document_search(query, top_k=top_k)\n",
    "        \n",
    "        # Graph-based entity search\n",
    "        graph_results = self._graph_enhanced_search(query, top_k=top_k)\n",
    "        \n",
    "        # Combine and re-rank results\n",
    "        combined_results = self._combine_search_results(text_results, graph_results)\n",
    "        \n",
    "        return combined_results[:top_k]\n",
    "    \n",
    "    def _placeholder_document_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Placeholder for document search - would integrate with actual document store\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"title\": f\"Document about {query}\",\n",
    "                \"content\": f\"This is a placeholder document discussing {query} and related financial topics.\",\n",
    "                \"source\": \"Financial Database\",\n",
    "                \"relevance_score\": 0.8,\n",
    "                \"search_type\": \"text_search\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def _graph_enhanced_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search using graph embeddings\"\"\"\n",
    "        \n",
    "        # Extract potential entities from query\n",
    "        entities_in_query = self._extract_query_entities(query)\n",
    "        \n",
    "        graph_results = []\n",
    "        for entity in entities_in_query:\n",
    "            similar_entities = self.kg_agent.find_similar_entities(\n",
    "                entity,\n",
    "                top_k=top_k,\n",
    "                filters={\"type\": \"company\"}  # Focus on companies for financial queries\n",
    "            )\n",
    "            \n",
    "            for similar_entity in similar_entities:\n",
    "                # Create a pseudo-document from graph entity\n",
    "                content = self._create_entity_document(similar_entity)\n",
    "                graph_results.append({\n",
    "                    \"title\": f\"Entity: {similar_entity['name']}\",\n",
    "                    \"content\": content,\n",
    "                    \"source\": \"Knowledge Graph\",\n",
    "                    \"relevance_score\": similar_entity[\"similarity_score\"],\n",
    "                    \"entity_data\": similar_entity,\n",
    "                    \"search_type\": \"graph_embedding\"\n",
    "                })\n",
    "        \n",
    "        return graph_results\n",
    "    \n",
    "    def _create_entity_document(self, entity_data: dict) -> str:\n",
    "        \"\"\"Create a document-like representation of a graph entity using SEC EDGAR data\"\"\"\n",
    "        \n",
    "        # Access financial data from the nested 'metadata' dictionary\n",
    "        metadata = entity_data.get('metadata', {})\n",
    "        \n",
    "        content_parts = [\n",
    "            f\"Company: {entity_data.get('name', 'N/A')}\",\n",
    "            f\"Ticker: {entity_data.get('ticker')}\" if entity_data.get('ticker') else \"\",\n",
    "            f\"Sector: {metadata.get('sector')}\" if metadata.get('sector') else \"\",\n",
    "            f\"Total Assets: ${metadata.get('total_assets', 0):,.0f}\" if metadata.get('total_assets', 0) > 0 else \"\",\n",
    "            f\"Revenue: ${metadata.get('revenues', 0):,.0f}\" if metadata.get('revenues', 0) > 0 else \"\",\n",
    "            f\"Net Income: ${metadata.get('net_income', 0):,.0f}\" if metadata.get('net_income', 0) > 0 else \"\",\n",
    "            f\"Market Cap: ${metadata.get('market_cap', 0):,.0f}\" if metadata.get('market_cap', 0) > 0 else \"\",\n",
    "            f\"P/E Ratio: {metadata.get('pe_ratio', 0):.2f}\" if metadata.get('pe_ratio', 0) > 0 else \"\",\n",
    "            f\"ROE: {metadata.get('return_on_equity', 0):.2f}%\" if metadata.get('return_on_equity', 0) > 0 else \"\",\n",
    "        ]\n",
    "        \n",
    "        return \" | \".join([part for part in content_parts if part])\n",
    "\n",
    "    \n",
    "    def _extract_query_entities(self, query: str) -> List[str]:\n",
    "        \"\"\"Extract potential entity names from query\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        # Company name patterns\n",
    "        company_patterns = [\n",
    "            r'\\b([A-Z][a-z]+ ?(?:Inc|Corp|LLC|Ltd|Company)\\.?)\\b',\n",
    "            r'\\b(Apple|Microsoft|Tesla|Amazon|Google|Meta|Netflix|Nvidia)\\b'\n",
    "        ]\n",
    "        \n",
    "        # Ticker patterns\n",
    "        ticker_pattern = r'\\b([A-Z]{2,5})\\b'\n",
    "        \n",
    "        for pattern in company_patterns:\n",
    "            entities.extend(re.findall(pattern, query, re.IGNORECASE))\n",
    "        \n",
    "        entities.extend(re.findall(ticker_pattern, query))\n",
    "        \n",
    "        return list(set(entities))\n",
    "    \n",
    "    def _combine_search_results(self, text_results: List[Dict], graph_results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Combine and re-rank text and graph search results\"\"\"\n",
    "        \n",
    "        combined = []\n",
    "        \n",
    "        # Add text results with weighted scores\n",
    "        for result in text_results:\n",
    "            result[\"combined_score\"] = result[\"relevance_score\"] * self.text_weight\n",
    "            combined.append(result)\n",
    "        \n",
    "        # Add graph results with weighted scores\n",
    "        for result in graph_results:\n",
    "            result[\"combined_score\"] = result[\"relevance_score\"] * self.graph_weight\n",
    "            combined.append(result)\n",
    "        \n",
    "        # Sort by combined score\n",
    "        combined.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "        \n",
    "        return combined\n",
    "\n",
    "# \n",
    "# ENHANCED FACT CHECKING SYSTEM\n",
    "# \n",
    "\n",
    "class EnhancedFactCheckerAgent:\n",
    "    \"\"\"Enhanced fact-checker using graph embeddings for verification\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SystemConfig, kg_agent: UnifiedKnowledgeGraphAgent):\n",
    "        self.config = config\n",
    "        self.kg_agent = kg_agent\n",
    "        self.cache = TTLCache(maxsize=1000, ttl=config.cache_ttl)\n",
    "    \n",
    "    def extract_claims(self, text: str, phi4_model) -> List[str]:\n",
    "        \"\"\"Extract claims from text using the LLM\"\"\"\n",
    "        return phi4_model.extract_claims(text)\n",
    "    \n",
    "    def verify_claim_with_embeddings(self, claim: str, entities: List[str]) -> FactCheckResult:\n",
    "        \"\"\"Verify claims using graph embedding similarity and relationships\"\"\"\n",
    "        \n",
    "        cache_key = hash(f\"{claim}_{str(entities)}\")\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        supporting_evidence = []\n",
    "        contradicting_evidence = []\n",
    "        confidence = 0.0\n",
    "        \n",
    "        # Extract entities mentioned in the claim\n",
    "        claim_entities = self._extract_claim_entities(claim, entities)\n",
    "        \n",
    "        if len(claim_entities) >= 2:\n",
    "            # Check relationship plausibility using embeddings\n",
    "            source_entity = claim_entities[0]\n",
    "            target_entity = claim_entities[1]\n",
    "            \n",
    "            # Find relationship path\n",
    "            relationship_path = self.kg_agent.find_relationship_path(source_entity, target_entity)\n",
    "            \n",
    "            if relationship_path:\n",
    "                path_confidence = relationship_path[\"confidence\"]\n",
    "                \n",
    "                if relationship_path[\"type\"] == \"direct\":\n",
    "                    supporting_evidence.append(\n",
    "                        f\"Direct relationship found: {' â†’ '.join(relationship_path['path'])}\"\n",
    "                    )\n",
    "                    confidence += 0.7 * path_confidence\n",
    "                else:\n",
    "                    supporting_evidence.append(\n",
    "                        f\"Inferred relationship via embeddings: {' â†’ '.join(relationship_path['relationships'])}\"\n",
    "                    )\n",
    "                    confidence += 0.4 * path_confidence\n",
    "            \n",
    "            # Cross-verify with peer companies using embeddings\n",
    "            peer_verification = self._verify_with_peer_companies(claim, source_entity)\n",
    "            confidence += peer_verification[\"confidence\"]\n",
    "            supporting_evidence.extend(peer_verification[\"evidence\"])\n",
    "        \n",
    "        # Numerical verification using similar companies\n",
    "        if self._contains_numbers(claim):\n",
    "            numerical_verification = self._verify_numerical_with_peers(claim, claim_entities)\n",
    "            confidence += numerical_verification[\"confidence\"]\n",
    "            supporting_evidence.extend(numerical_verification[\"evidence\"])\n",
    "        \n",
    "        result = FactCheckResult(\n",
    "            claim=claim,\n",
    "            verified=confidence >= self.config.fact_check_threshold,\n",
    "            confidence=min(confidence, 1.0),\n",
    "            supporting_evidence=supporting_evidence,\n",
    "            contradicting_evidence=contradicting_evidence,\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "        \n",
    "        self.cache[cache_key] = result\n",
    "        return result\n",
    "    \n",
    "    def _verify_with_peer_companies(self, claim: str, entity: str) -> Dict[str, Any]:\n",
    "        \"\"\"Verify claim against peer companies using graph embeddings\"\"\"\n",
    "        \n",
    "        # Find similar companies\n",
    "        peer_companies = self.kg_agent.find_similar_entities(\n",
    "            entity,\n",
    "            top_k=5,\n",
    "            filters={\"type\": \"company\"}\n",
    "        )\n",
    "        \n",
    "        evidence = []\n",
    "        confidence = 0.0\n",
    "        \n",
    "        for peer in peer_companies:\n",
    "            if peer[\"similarity_score\"] > 0.8:  # High similarity threshold\n",
    "                evidence.append(f\"Similar pattern found in peer company: {peer['name']} (similarity: {peer['similarity_score']:.2f})\")\n",
    "                confidence += 0.1\n",
    "        \n",
    "        return {\"evidence\": evidence, \"confidence\": min(confidence, 0.5)}\n",
    "    \n",
    "    def _verify_numerical_with_peers(self, claim: str, entities: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Verify numerical claims against peer group averages\"\"\"\n",
    "        \n",
    "        evidence = []\n",
    "        confidence = 0.0\n",
    "        \n",
    "        if not entities:\n",
    "            return {\"evidence\": evidence, \"confidence\": confidence}\n",
    "        \n",
    "        primary_entity = entities[0]\n",
    "        \n",
    "        # Get peer companies\n",
    "        peers = self.kg_agent.find_similar_entities(\n",
    "            primary_entity,\n",
    "            top_k=10,\n",
    "            filters={\"type\": \"company\"}\n",
    "        )\n",
    "        \n",
    "        # Extract numerical values from claim\n",
    "        numbers = re.findall(r'\\d+\\.?\\d*', claim)\n",
    "        if not numbers:\n",
    "            return {\"evidence\": evidence, \"confidence\": confidence}\n",
    "        \n",
    "        claimed_value = float(numbers[0])\n",
    "        \n",
    "        # Check if value is reasonable compared to peers\n",
    "        if 'pe ratio' in claim.lower() or 'p/e' in claim.lower():\n",
    "            peer_pe_ratios = [p[\"pe_ratio\"] for p in peers if p[\"pe_ratio\"] > 0]\n",
    "            if peer_pe_ratios:\n",
    "                avg_pe = np.mean(peer_pe_ratios)\n",
    "                std_pe = np.std(peer_pe_ratios)\n",
    "                \n",
    "                if abs(claimed_value - avg_pe) <= 2 * std_pe:\n",
    "                    evidence.append(f\"P/E ratio {claimed_value} within reasonable range of peer average {avg_pe:.2f}\")\n",
    "                    confidence += 0.3\n",
    "                else:\n",
    "                    evidence.append(f\"P/E ratio {claimed_value} outside peer range (avg: {avg_pe:.2f})\")\n",
    "        \n",
    "        return {\"evidence\": evidence, \"confidence\": confidence}\n",
    "    \n",
    "    def _extract_claim_entities(self, claim: str, entities: List[str]) -> List[str]:\n",
    "        \"\"\"Extract entities specifically mentioned in the claim\"\"\"\n",
    "        claim_entities = []\n",
    "        for entity in entities:\n",
    "            if entity.lower() in claim.lower():\n",
    "                claim_entities.append(entity)\n",
    "        return claim_entities\n",
    "    \n",
    "    def _contains_numbers(self, text: str) -> bool:\n",
    "        \"\"\"Check if text contains numerical values\"\"\"\n",
    "        return bool(re.search(r'\\d+\\.?\\d*', text))\n",
    "\n",
    "\n",
    "class FinancialGraphRAGQueryEngine:\n",
    "    \"\"\"A query engine that uses GraphRAG principles for financial analysis.\"\"\"\n",
    "\n",
    "    def __init__(self, kg_agent: UnifiedKnowledgeGraphAgent, phi4_agent: FineTunedPhi4Agent, communities: dict, sec_api_tool: SECAPITool):\n",
    "        self.kg_agent = kg_agent\n",
    "        self.phi4_agent = phi4_agent\n",
    "        self.communities = communities\n",
    "        self.e5_embedder = self.kg_agent.embedding_model # Access the embedder\n",
    "        self.sec_api_tool = sec_api_tool\n",
    "\n",
    "        # Create a searchable index for the community summaries\n",
    "        self._initialize_community_index()\n",
    "\n",
    "    def _initialize_community_index(self):\n",
    "        \"\"\"Creates a FAISS index for the natural language summaries of each community.\"\"\"\n",
    "        if not self.communities:\n",
    "            logger.warning(\"No communities available. Creating empty community index.\")\n",
    "            # Create a minimal FAISS index with correct dimensions but no vectors\n",
    "            self.community_index = faiss.IndexFlatL2(1024)  # E5 embedding dimension\n",
    "            self.community_id_map = []\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Initializing FAISS index for {len(self.communities)} community summaries.\")\n",
    "        summaries = [data['summary'] for data in self.communities.values()]\n",
    "        \n",
    "        # Check if summaries is empty before encoding\n",
    "        if not summaries:\n",
    "            logger.warning(\"No community summaries available. Creating empty community index.\")\n",
    "            self.community_index = faiss.IndexFlatL2(1024)\n",
    "            self.community_id_map = []\n",
    "            return\n",
    "        \n",
    "        # Use the existing E5 embedder to create vectors for the summaries\n",
    "        summary_vectors = self.e5_embedder.encode_passages(summaries).astype('float32')\n",
    "        \n",
    "        self.community_index = faiss.IndexFlatL2(summary_vectors.shape[1])\n",
    "        self.community_index.add(summary_vectors)\n",
    "        \n",
    "        # Map the index position back to the original community ID\n",
    "        self.community_id_map = list(self.communities.keys())\n",
    "        logger.info(\"Community index created successfully.\")\n",
    "\n",
    "\n",
    "    def _get_relevant_communities(self, question: str, top_k: int = 3) -> dict:\n",
    "        \"\"\"Finds the most relevant communities for a global question.\"\"\"\n",
    "        question_vector = self.e5_embedder.encode_queries([question])\n",
    "        _, indices = self.community_index.search(question_vector, top_k)\n",
    "        \n",
    "        relevant_communities = {}\n",
    "        for idx in indices[0]:\n",
    "            community_id = self.community_id_map[idx]\n",
    "            relevant_communities[community_id] = self.communities[community_id]\n",
    "        return relevant_communities\n",
    "\n",
    "    def _get_entity_neighbors(self, entity_name: str, top_k: int = 5) -> List[dict]:\n",
    "        \"\"\"Finds related entities (peers) for a local query, simulating graph traversal.\"\"\"\n",
    "        return self.kg_agent.get_peer_companies(entity_name, top_k=top_k)\n",
    "\n",
    "    \n",
    "    def _build_entity_context(self, entities: List[dict]) -> str:\n",
    "        \"\"\"Creates a clean, human-readable text block from a list of entities.\"\"\"\n",
    "        context_lines = []\n",
    "        for entity in entities: \n",
    "    \n",
    "            # First, safely get the nested metadata dictionary\n",
    "            metadata = entity.get('metadata', {})\n",
    "            \n",
    "            # Now, access all data from the correct locations\n",
    "            name = entity.get('name', 'N/A')\n",
    "            # The ticker may be at the top level or in metadata, so check both\n",
    "            ticker = entity.get('ticker') or (metadata.get('tickers', ['N/A'])[0] if metadata.get('tickers') else 'N/A')\n",
    "            sector = metadata.get('sector', 'N/A')\n",
    "            # Use the correct keys for market cap and P/E ratio as defined in the graph\n",
    "            mkt_cap = metadata.get('mkt_market_cap', 0) \n",
    "            pe_ratio = metadata.get('mkt_pe_ratio', 0)\n",
    "            \n",
    "            \n",
    "            # Format market cap for readability\n",
    "            if mkt_cap > 1e12:\n",
    "                mkt_cap_str = f\"${mkt_cap / 1e12:.2f}T\"\n",
    "            elif mkt_cap > 1e9:\n",
    "                mkt_cap_str = f\"${mkt_cap / 1e9:.2f}B\"\n",
    "            elif mkt_cap > 1e6:\n",
    "                mkt_cap_str = f\"${mkt_cap / 1e6:.2f}M\"\n",
    "            else:\n",
    "                mkt_cap_str = f\"${mkt_cap:,.2f}\"\n",
    "    \n",
    "            line = f\"- {name} ({ticker}): Sector={sector}, Market Cap={mkt_cap_str}, P/E Ratio={pe_ratio:.2f}\"\n",
    "            context_lines.append(line)\n",
    "            \n",
    "        return \"\\n\".join(context_lines)\n",
    "\n",
    "    def global_query(self, question: str, entities_from_llm: Optional[List[dict]] = None) -> str:\n",
    "        \"\"\"\n",
    "        ENHANCED: Handles direct data questions with Yahoo Finance integration - NO LLM for final response\n",
    "        \"\"\"\n",
    "        if not entities_from_llm:\n",
    "            return None\n",
    "    \n",
    "        entity = entities_from_llm[0]\n",
    "        metadata = entity.get('metadata', {})\n",
    "        entity_name = entity.get('name', 'Unknown Company')\n",
    "        cik = metadata.get('cik')\n",
    "    \n",
    "        available_facts = {}\n",
    "        source_description = \"\"\n",
    "        context_entity_name = entity_name\n",
    "    \n",
    "        # Step 1: Try comprehensive live data (SEC + Yahoo Finance)\n",
    "        if cik:\n",
    "            logger.info(f\"Attempting live data fetch for {entity_name} (CIK: {cik})...\")\n",
    "            live_data = self.sec_api_tool.get_live_company_data(cik)\n",
    "            \n",
    "            if live_data:\n",
    "                logger.info(f\" Successfully fetched live data for {entity_name}\")\n",
    "                context_entity_name = live_data.get('entityName', entity_name)\n",
    "                source_type = live_data.get('data_source', 'SEC')\n",
    "                \n",
    "                # Store raw data for direct access\n",
    "                available_facts = {k: v for k, v in live_data.items() \n",
    "                                if isinstance(v, (int, float)) and k not in ['data_source']}\n",
    "                source_description = f\"Live data from {source_type}\"\n",
    "    \n",
    "        # Step 2: If no CIK available, try direct Yahoo Finance lookup\n",
    "        if not available_facts and entity_name != 'Unknown Company':\n",
    "            \n",
    "            yahoo_data = self.sec_api_tool.get_yahoo_finance_data(entity_name)\n",
    "            \n",
    "            if yahoo_data:\n",
    "                logger.info(f\" Successfully fetched Yahoo Finance data for {entity_name}\")\n",
    "                context_entity_name = yahoo_data.get('entityName', entity_name)\n",
    "                \n",
    "                available_facts = {k: v for k, v in yahoo_data.items() \n",
    "                                if isinstance(v, (int, float)) and k not in ['data_source']}\n",
    "                source_description = \"Live data from Yahoo Finance\"\n",
    "    \n",
    "        # Step 3: Fall back to stored KG data if all else fails\n",
    "        if not available_facts:\n",
    "            logger.warning(f\"Live data unavailable. Falling back to stored KG data for {entity_name}.\")\n",
    "            stored_facts = {\n",
    "                \"P/E Ratio\": metadata.get('mkt_pe_ratio'), \n",
    "                \"Market Cap\": metadata.get('mkt_market_cap'),\n",
    "                \"Revenue\": metadata.get('revenues'), \n",
    "                \"Total Assets\": metadata.get('total_assets'),\n",
    "            }\n",
    "            available_facts = {k: v for k, v in stored_facts.items() if isinstance(v, (int, float)) and v != 0}\n",
    "            source_description = \"Stored knowledge graph data\"\n",
    "    \n",
    "        # Step 4: Direct response without LLM\n",
    "        if not available_facts:\n",
    "            return f\"I could not retrieve financial data for {entity_name} from any available source.\"\n",
    "    \n",
    "        # Parse question and return specific data directly\n",
    "        return self._extract_specific_data(question, available_facts, context_entity_name, source_description)\n",
    "    \n",
    "    def _extract_specific_data(self, question: str, facts: dict, entity_name: str, source: str) -> str:\n",
    "        \"\"\"Extract and return specific data based on the question without LLM processing\"\"\"\n",
    "        \n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Define mapping of question patterns to data keys\n",
    "        data_mappings = {\n",
    "            # P/E Ratio variations\n",
    "            ('pe ratio', 'p/e ratio', 'pe', 'price to earnings'): ['P/E Ratio', 'pe_ratio', 'mkt_pe_ratio'],\n",
    "            \n",
    "            # Market Cap variations  \n",
    "            ('market cap', 'market capitalization', 'market value'): ['Market Cap', 'market_cap', 'mkt_market_cap'],\n",
    "            \n",
    "            # Price variations\n",
    "            ('price', 'stock price', 'current price', 'share price'): ['Current Price', 'current_price', 'mkt_current_price'],\n",
    "            \n",
    "            # Revenue variations\n",
    "            ('revenue', 'revenues', 'sales', 'total revenue'): ['Revenue', 'revenues', 'totalRevenue'],\n",
    "            \n",
    "            # Other ratios\n",
    "            ('pb ratio', 'p/b ratio', 'price to book'): ['P/B Ratio', 'pb_ratio', 'mkt_pb_ratio'],\n",
    "            ('dividend yield', 'dividend'): ['Dividend Yield', 'dividend_yield', 'dividendYield'],\n",
    "            ('beta',): ['Beta', 'beta'],\n",
    "            ('roe', 'return on equity'): ['ROE', 'roe', 'returnOnEquity'],\n",
    "            \n",
    "            # Financial metrics\n",
    "            ('total assets', 'assets'): ['Total Assets', 'total_assets', 'totalAssets'],\n",
    "            ('total debt', 'debt'): ['Total Debt', 'total_debt', 'totalDebt'],\n",
    "            ('net income', 'profit', 'earnings'): ['Net Income', 'net_income', 'netIncome'],\n",
    "        }\n",
    "        \n",
    "        # Find matching data\n",
    "        for question_patterns, data_keys in data_mappings.items():\n",
    "            if any(pattern in question_lower for pattern in question_patterns):\n",
    "                # Look for the data in facts\n",
    "                for key in data_keys:\n",
    "                    if key in facts and facts[key] is not None:\n",
    "                        value = facts[key]\n",
    "                        \n",
    "                        # Format the response based on data type\n",
    "                        formatted_value = self._format_financial_value(key, value)\n",
    "                        \n",
    "                        return f\"{entity_name}'s {question_patterns[0]} is {formatted_value}. (Source: {source})\"\n",
    "        \n",
    "        # If no specific match, list available data\n",
    "        available_metrics = []\n",
    "        for key, value in list(facts.items())[:10]:  # Limit to first 10 items\n",
    "            formatted_value = self._format_financial_value(key, value)\n",
    "            available_metrics.append(f\"{key}: {formatted_value}\")\n",
    "        \n",
    "        return f\"Available data for {entity_name}:\\n\" + \"\\n\".join(available_metrics) + f\"\\n\\n(Source: {source})\"\n",
    "    \n",
    "    def _format_financial_value(self, key: str, value: float) -> str:\n",
    "        \"\"\"Format financial values appropriately\"\"\"\n",
    "        \n",
    "        # Handle ratios and percentages (values between 0 and 1 that aren't prices)\n",
    "        if isinstance(value, float) and 0 < abs(value) < 1:\n",
    "            if any(term in key.lower() for term in ['ratio', 'margin', 'yield', 'roe', 'roa']):\n",
    "                if 'yield' in key.lower() or 'margin' in key.lower():\n",
    "                    return f\"{value:.2%}\"  # Display as percentage\n",
    "                else:\n",
    "                    return f\"{value:.2f}\"  # Display as decimal\n",
    "            else:\n",
    "                return f\"{value:.4f}\"\n",
    "        \n",
    "        # Handle large monetary values\n",
    "        elif isinstance(value, (int, float)) and abs(value) >= 1000000000:\n",
    "            return f\"${value/1e9:.2f}B\"\n",
    "        elif isinstance(value, (int, float)) and abs(value) >= 1000000:\n",
    "            return f\"${value/1e6:.2f}M\"\n",
    "        elif isinstance(value, (int, float)) and abs(value) >= 1000:\n",
    "            return f\"${value:,.2f}\"\n",
    "        \n",
    "        # Handle regular numbers\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return f\"{value:,.2f}\"\n",
    "        \n",
    "        return str(value)\n",
    "\n",
    "\n",
    "    def local_query(self, question: str, entities_from_llm: Optional[List[dict]] = None, gnn_insights: Optional[dict] = None) -> str:\n",
    "        entities = entities_from_llm or []\n",
    "        if not entities:\n",
    "            return \"Please specify a company for analysis.\"\n",
    "    \n",
    "        primary_entity = entities[0]\n",
    "        entity_name = primary_entity.get('name', 'Unknown Company')\n",
    "        \n",
    "        # Check for competitor/similarity queries\n",
    "        similarity_keywords = ['similar', 'like', 'competitors', 'compete', 'peers', 'vs']\n",
    "        if any(word in question.lower() for word in similarity_keywords):\n",
    "            \n",
    "            # PRIORITY 1: Use GNN insights if available\n",
    "            if gnn_insights and gnn_insights.get('similar_companies'):\n",
    "                similar_companies = gnn_insights['similar_companies']\n",
    "                response_lines = [f\"Based on graph neural network analysis, here are the top competitors for {entity_name}:\\n\"]\n",
    "                for i, comp in enumerate(similar_companies[:10], 1):\n",
    "                    sector = comp.get('sector', 'N/A')\n",
    "                    score = comp.get('similarity_score', 0)\n",
    "                    response_lines.append(f\"{i}. {comp['name']} (Sector: {sector}, Similarity: {score:.3f})\")\n",
    "                return \"\\n\".join(response_lines)\n",
    "            \n",
    "            # FALLBACK: Use E5 semantic search with enhanced query\n",
    "            # ... your existing E5 logic here\n",
    "            else:\n",
    "                # 1. Get all available descriptive data from the entity's metadata\n",
    "                entity_name = primary_entity.get('name', 'Unknown Company')\n",
    "                metadata = primary_entity.get('metadata', {})\n",
    "                sector = metadata.get('sector', '')\n",
    "                sic_description = metadata.get('sic_description', '')\n",
    "    \n",
    "                # 2. Build a list of meaningful keywords. This is more robust than a fixed sentence.\n",
    "                query_keywords = [entity_name]\n",
    "                if sector and sector.lower() != 'other':\n",
    "                    query_keywords.append(sector)\n",
    "                if sic_description and sic_description.lower() not in ['company', '']:\n",
    "                    # Add the description only if it's specific and not a generic fallback\n",
    "                    query_keywords.append(sic_description)\n",
    "    \n",
    "                # Join keywords to form the final query string\n",
    "                enriched_query = \" \".join(query_keywords)\n",
    "                logger.info(f\" Semantic competitor query detected. Using keyword-based query for E5 search: '{enriched_query}'\")\n",
    "    \n",
    "                # 3. Perform the semantic search with the new, stronger query\n",
    "                similar_companies = self.kg_agent.find_similar_entities(\n",
    "                    enriched_query,\n",
    "                    model_type='e5',\n",
    "                    top_k=11,\n",
    "                    filters={\"type\": \"company\"}\n",
    "                )\n",
    "                \n",
    "                if not similar_companies:\n",
    "                    return f\"Could not find any competitors for {entity_name} using the enriched query.\"\n",
    "    \n",
    "                # 4. Filter and format the response (same as before)\n",
    "                response_lines = [f\"Based on semantic analysis, here are the top competitors for {entity_name}:\\n\"]\n",
    "                seen_names = {entity_name.strip().lower()}\n",
    "                count = 0\n",
    "                \n",
    "                for company in similar_companies:\n",
    "                    comp_name = company.get('name', '').strip()\n",
    "                    if comp_name and comp_name.lower() not in seen_names:\n",
    "                        response_lines.append(f\"{count + 1}. {comp_name}\")\n",
    "                        seen_names.add(comp_name.lower())\n",
    "                        count += 1\n",
    "                    if count >= 10:\n",
    "                        break\n",
    "                \n",
    "                if count == 0:\n",
    "                    return f\"No distinct competitors found for {entity_name} after filtering.\"\n",
    "                    \n",
    "                return \"\\n\".join(response_lines)\n",
    "        \n",
    "        # Fallback for other non-competitor \"local\" queries.\n",
    "        logger.info(f\"Handling other local query. Using GNN insights for {primary_entity.get('name', 'Unknown')}.\")\n",
    "        if gnn_insights and gnn_insights.get('similar_companies'):\n",
    "            similar_companies = gnn_insights['similar_companies']\n",
    "            lines = [f\"Structurally similar companies to {primary_entity.get('name', 'Unknown')} (from GNN):\\n\"]\n",
    "            for i, comp in enumerate(similar_companies[:10], 1):\n",
    "                lines.append(f\"{i}. {comp['name']} (Score: {comp['similarity_score']:.2f})\")\n",
    "            return \"\\n\".join(lines)\n",
    "        else:\n",
    "            return f\"Unable to perform the requested local analysis for {primary_entity.get('name', 'Unknown')}.\"\n",
    "\n",
    "    \n",
    "    def local_query(self, question: str, entities_from_llm: Optional[List[dict]] = None, gnn_insights: Optional[dict] = None) -> str:\n",
    "        \"\"\"\n",
    "        REVISED: This method builds a robust keyword-based query for semantic search,\n",
    "        making it resilient to missing or poor-quality data points.\n",
    "        \"\"\"\n",
    "        entities = entities_from_llm or []\n",
    "        if not entities:\n",
    "            return \"Please specify a company for analysis.\"\n",
    "\n",
    "        primary_entity = entities[0]\n",
    "        \n",
    "        # Build a keyword-based query \n",
    "        similarity_keywords = ['similar', 'like', 'competitors', 'compete', 'peers', 'vs']\n",
    "        if any(word in question.lower() for word in similarity_keywords):\n",
    "            \n",
    "            # 1. Get all available descriptive data from the entity's metadata\n",
    "            entity_name = primary_entity.get('name', 'Unknown Company')\n",
    "            metadata = primary_entity.get('metadata', {})\n",
    "            sector = metadata.get('sector', '')\n",
    "            sic_description = metadata.get('sic_description', '')\n",
    "\n",
    "            # 2. Build a list of meaningful keywords. This is more robust than a fixed sentence.\n",
    "            query_keywords = [entity_name]\n",
    "            if sector and sector.lower() != 'other':\n",
    "                query_keywords.append(sector)\n",
    "            if sic_description and sic_description.lower() not in ['company', '']:\n",
    "                # Add the description only if it's specific and not a generic fallback\n",
    "                query_keywords.append(sic_description)\n",
    "\n",
    "            # Join keywords to form the final query string\n",
    "            enriched_query = \" \".join(query_keywords)\n",
    "            logger.info(f\" Semantic competitor query detected. Using keyword-based query for E5 search: '{enriched_query}'\")\n",
    "\n",
    "            # 3. Perform the semantic search with the new, stronger query\n",
    "            similar_companies = self.kg_agent.find_similar_entities(\n",
    "                enriched_query,\n",
    "                model_type='e5',\n",
    "                top_k=11,\n",
    "                filters={\"type\": \"company\"}\n",
    "            )\n",
    "            \n",
    "            if not similar_companies:\n",
    "                return f\"Could not find any competitors for {entity_name} using the enriched query.\"\n",
    "\n",
    "            # 4. Filter and format the response (same as before)\n",
    "            response_lines = [f\"Based on semantic analysis, here are the top competitors for {entity_name}:\\n\"]\n",
    "            seen_names = {entity_name.strip().lower()}\n",
    "            count = 0\n",
    "            \n",
    "            for company in similar_companies:\n",
    "                comp_name = company.get('name', '').strip()\n",
    "                if comp_name and comp_name.lower() not in seen_names:\n",
    "                    response_lines.append(f\"{count + 1}. {comp_name}\")\n",
    "                    seen_names.add(comp_name.lower())\n",
    "                    count += 1\n",
    "                if count >= 10:\n",
    "                    break\n",
    "            \n",
    "            if count == 0:\n",
    "                return f\"No distinct competitors found for {entity_name} after filtering.\"\n",
    "                \n",
    "            return \"\\n\".join(response_lines)\n",
    "       \n",
    "    \n",
    "# \n",
    "# GNN MODEL DEFINITION (SYNCHRONIZED WITH TRAINING SCRIPT)\n",
    "# \n",
    "class AttentionBasedFinancialGNN(nn.Module):\n",
    "    \"\"\" Architecture matching the training script exactly\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=128, num_heads=8, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            in_channels = input_dim if i == 0 else hidden_dim  \n",
    "            concat = i < num_layers - 1\n",
    "            \n",
    "            self.attention_layers.append(\n",
    "                GATv2Conv(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=hidden_dim // num_heads,  # 128 // 8 = 16\n",
    "                    heads=num_heads,\n",
    "                    dropout=0.1,\n",
    "                    concat=concat  # Only final layer doesn't concatenate\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Final embedding dimension matches training script\n",
    "        final_embedding_dim = hidden_dim // num_heads  # 16, not 128\n",
    "        self.feature_predictor = nn.Linear(final_embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, return_attention=False):\n",
    "        for i, layer in enumerate(self.attention_layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i < len(self.attention_layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.1, training=self.training)\n",
    "        \n",
    "        return {\n",
    "            'embeddings': x,\n",
    "            'feature_prediction': self.feature_predictor(x).squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "# \n",
    "# LANGGRAPH NODES (UPDATED)\n",
    "# \n",
    "\n",
    "\n",
    "def query_analysis_node(state: FinancialState, phi4_agent: FineTunedPhi4Agent) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    FINAL VERSION: Analyzes the query, robustly extracts entities, and then resolves\n",
    "    only those extracted entities against the knowledge graph.\n",
    "    \"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    kg_agent = state['kg_agent']\n",
    "    logger.info(\" Entering Node: query_analysis \")\n",
    "    \n",
    "    # Step 1: Use the LLM to extract potential entity names from the query.\n",
    "    analysis = phi4_agent.analyze_query(query)\n",
    "    \n",
    "    resolved_entities = []\n",
    "    if analysis.get(\"entities\"):\n",
    "        all_resolved_entities = []\n",
    "\n",
    "        # Step 2: Loop through ONLY the names extracted by the LLM.\n",
    "        for entity_name in analysis[\"entities\"]:\n",
    "            clean_name = re.sub(r\"'s\\b\", \"\", entity_name).strip()\n",
    "            if not clean_name:\n",
    "                continue\n",
    "\n",
    "            # This sanity check is still valuable.\n",
    "            if len(clean_name.split()) > 6:\n",
    "                logger.warning(f\"Skipping resolution for implausible entity name: '{clean_name}'\")\n",
    "                continue\n",
    "\n",
    "            # Step 3: Resolve the CLEANED NAME, not the whole query. \n",
    "            resolved = kg_agent.resolve_entity(clean_name)\n",
    "            \n",
    "            if resolved:\n",
    "                logger.info(f\" Resolved '{clean_name}' to '{resolved[0]['name']}'.\")\n",
    "                all_resolved_entities.extend(resolved)\n",
    "        \n",
    "        # De-duplicate and finalize the list of resolved entities\n",
    "        if all_resolved_entities:\n",
    "            unique_entities_dict = {\n",
    "                (e.get('metadata', {}).get('cik') or e.get('id')): e for e in all_resolved_entities\n",
    "            }\n",
    "            resolved_entities = list(unique_entities_dict.values())\n",
    "            analysis[\"entities\"] = [e['name'] for e in resolved_entities]\n",
    "\n",
    "    if not resolved_entities and analysis.get(\"query_type\") == \"local\":\n",
    "        logger.warning(f\"A 'local' query was identified, but no entities could be resolved.\")\n",
    "\n",
    "    return {\n",
    "        \"analysis_results\": analysis,\n",
    "        \"entities\": resolved_entities\n",
    "    }\n",
    "    \n",
    "def gnn_inference_node(state: FinancialState, gnn_model: nn.Module, kg: Any) -> Dict[str, Any]:\n",
    "    \"\"\" GNN-based similarity search using neighborhood sampling.\"\"\"\n",
    "    if not gnn_model or not kg:\n",
    "        return {\"gnn_analysis\": {\"status\": \"GNN model or graph not loaded. Skipping.\"}}\n",
    "        \n",
    "    entities = state.get(\"entities\", [])\n",
    "    if not entities:\n",
    "        return {\"gnn_analysis\": {\"status\": \"No entities for GNN analysis.\"}}\n",
    "\n",
    "    target_entity = entities[0]\n",
    "    \n",
    "    # 1: Better target node ID resolution\n",
    "    target_node_id = None\n",
    "    metadata = target_entity.get('metadata', {})\n",
    "    \n",
    "    # Try different ID fields in order of preference\n",
    "    for id_field in ['node_id', 'cik', 'id']:\n",
    "        if id_field in metadata and metadata[id_field]:\n",
    "            target_node_id = str(metadata[id_field])\n",
    "            break\n",
    "    \n",
    "    # If still no ID found, try top-level fields\n",
    "    if not target_node_id:\n",
    "        for id_field in ['cik', 'id']:\n",
    "            if id_field in target_entity and target_entity[id_field]:\n",
    "                target_node_id = str(target_entity[id_field])\n",
    "                break\n",
    "    \n",
    "    if not target_node_id:\n",
    "        return {\"gnn_analysis\": {\"status\": \"Target entity node ID not found in any expected field.\"}}\n",
    "\n",
    "    node_list = list(kg.graph.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    if target_node_id not in node_to_idx:\n",
    "        logger.warning(f\"Target node ID '{target_node_id}' not found in graph. Available IDs sample: {list(node_to_idx.keys())[:5]}\")\n",
    "        return {\"gnn_analysis\": {\"status\": f\"Target entity {target_node_id} not found in graph.\"}}\n",
    "    \n",
    "    # Create the PyG Data object\n",
    "    features = [kg.get_enhanced_node_features(n) for n in node_list]\n",
    "    x = torch.tensor(np.array(features), dtype=torch.float)\n",
    "    edges = torch.tensor([[node_to_idx[u], node_to_idx[v]] for u, v in kg.graph.edges()], dtype=torch.long).t().contiguous()\n",
    "    data = Data(x=x, edge_index=edges, num_nodes=len(node_list))\n",
    "    \n",
    "    target_idx = node_to_idx[target_node_id]\n",
    "    \n",
    "    neighbor_loader = NeighborLoader(\n",
    "        data,\n",
    "        input_nodes=torch.tensor([target_idx]),\n",
    "        num_neighbors=[15, 10],\n",
    "        batch_size=1,\n",
    "        replace=False,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    logger.info(f\" Running memory-efficient GNN similarity search for: {target_entity['name']} (ID: {target_node_id})\")\n",
    "    \n",
    "    sampled_data = next(iter(neighbor_loader))\n",
    "    subgraph_x = sampled_data.x.to(device)\n",
    "    subgraph_edge_index = sampled_data.edge_index.to(device)\n",
    "    \n",
    "    logger.info(f\" Subgraph size: {subgraph_x.shape[0]} nodes (vs full graph: {len(node_list)})\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = gnn_model(subgraph_x, subgraph_edge_index)\n",
    "        embeddings = output['embeddings']\n",
    "    \n",
    "    # 2: Correctly identify the target entity in the subgraph\n",
    "    # The target entity should be at index 0 in the subgraph node mapping\n",
    "    target_subgraph_idx = 0  # NeighborLoader puts the input node first\n",
    "    target_embedding = embeddings[target_subgraph_idx].unsqueeze(0)\n",
    "    \n",
    "    # Calculate similarities with all nodes in the subgraph\n",
    "    similarities = F.cosine_similarity(target_embedding, embeddings, dim=1)\n",
    "    \n",
    "    # 3: Properly exclude the target entity and filter for companies only\n",
    "    similar_companies = []\n",
    "    \n",
    "    for subgraph_idx in range(len(similarities)):\n",
    "        if subgraph_idx == target_subgraph_idx:\n",
    "            continue  # Skip the target entity itself\n",
    "            \n",
    "        similarity_score = similarities[subgraph_idx].cpu().item()\n",
    "        \n",
    "        # Map back to original node\n",
    "        original_node_idx = sampled_data.n_id[subgraph_idx].item()\n",
    "        node_id = node_list[original_node_idx]\n",
    "        node_attrs = kg.graph.nodes[node_id]\n",
    "        \n",
    "        # 4: Only include companies and add sector filtering\n",
    "        if node_attrs.get('type') == 'company':\n",
    "            target_sector = kg.graph.nodes[target_node_id].get('sector', '')\n",
    "            node_sector = node_attrs.get('sector', '')\n",
    "            \n",
    "            # Boost similarity for same-sector companies\n",
    "            if target_sector and node_sector == target_sector:\n",
    "                similarity_score *= 1.2  # Boost same-sector similarity\n",
    "            \n",
    "            similar_companies.append({\n",
    "                'name': node_attrs.get('name', 'Unknown'),\n",
    "                'node_id': node_id,\n",
    "                'similarity_score': float(similarity_score),\n",
    "                'sector': node_sector,\n",
    "                'ticker': node_attrs.get('tickers', [''])[0] if node_attrs.get('tickers') else '',\n",
    "            })\n",
    "    \n",
    "    # 5: Sort by similarity and take top results\n",
    "    similar_companies.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "    similar_companies = similar_companies[:10]  # Take top 10\n",
    "    \n",
    "    feature_predictions = output['feature_prediction'].cpu()\n",
    "    if len(feature_predictions.shape) == 0:  # Single scalar\n",
    "        pe_prediction = feature_predictions.item()\n",
    "    else:  # Multiple predictions - take the target node (index 0)\n",
    "        pe_prediction = feature_predictions[0].item()\n",
    "    \n",
    "    analysis_result = {\n",
    "        'target_entity': target_entity['name'],\n",
    "        'target_node_id': target_node_id,\n",
    "        'similar_companies': similar_companies,\n",
    "        'predicted_pe_ratio': float(pe_prediction),\n",
    "        'method': 'gnn_neighborhood_sampling_fixed'\n",
    "    }\n",
    "    \n",
    "    logger.info(f\" Found {len(similar_companies)} similar companies for {target_entity['name']}\")\n",
    "    if similar_companies:\n",
    "        # 1. First, create the list of formatted strings\n",
    "        top_matches_list = [f\"{c['name']} ({c['similarity_score']:.3f})\" for c in similar_companies[:3]]\n",
    "        # 2. Then, use the created list in the log message\n",
    "        # logger.info(f\"Top 3 matches: {top_matches_list}\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\"gnn_analysis\": analysis_result}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def numerical_reasoning_node(state: FinancialState, numerical_agent: EnhancedNumericalReasoning) -> Dict[str, Any]:\n",
    "    \"\"\"Handles numerical queries using the two-stage reasoning agent.\"\"\"\n",
    "    logger.info(\" Entering Node: numerical_reasoning \")\n",
    "    query = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Perform the two-stage reasoning\n",
    "    result = numerical_agent.numerical_query_with_steps(query)\n",
    "    \n",
    "    # Format the final response for the user\n",
    "    if \"error\" in result:\n",
    "        response_text = result[\"error\"]\n",
    "    else:\n",
    "        final_answer = result.get('final_result', {}).get('answer', 'Could not compute the final answer.')\n",
    "        reasoning_steps = result.get('intermediate_steps', {}).get('reasoning', \"No reasoning provided.\")\n",
    "        calculations = result.get('final_result', {}).get('final_calculations', [])\n",
    "        \n",
    "        response_text = (\n",
    "            f\"Final Answer: {final_answer}\\n\\n\"\n",
    "            f\" Reasoning \\n{reasoning_steps}\\n\\n\"\n",
    "            f\" Calculation Steps \\n\" + \"\\n\".join(calculations)\n",
    "        )\n",
    "            \n",
    "    return {\"messages\": [AIMessage(content=response_text)]}\n",
    "\n",
    "\n",
    "\n",
    "def graphrag_analysis_node(state: FinancialState, graphrag_engine: Any) -> Dict[str, Any]:\n",
    "    \"\"\"Generate response using GraphRAG with LLM-extracted entities.\"\"\"\n",
    "    try:\n",
    "        query = state[\"messages\"][-1].content\n",
    "        entities = state.get(\"entities\", [])  #  LLM-extracted entities\n",
    "        gnn_analysis = state.get(\"gnn_analysis\", {})\n",
    "        query_type = state.get(\"analysis_results\", {}).get(\"query_type\", \"global\")\n",
    "        \n",
    "        if query_type == \"global\":\n",
    "            # Pass LLM-extracted entities to global_query\n",
    "            response = graphrag_engine.global_query(\n",
    "                question=query,\n",
    "                entities_from_llm=entities\n",
    "            )\n",
    "        else:\n",
    "            #  Pass LLM-extracted entities to local_query\n",
    "            response = graphrag_engine.local_query(\n",
    "                question=query,\n",
    "                entities_from_llm=entities,  # Use LLM entities\n",
    "                gnn_insights=gnn_analysis\n",
    "            )\n",
    "\n",
    "        return {\"messages\": [AIMessage(content=response)]}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error in graphrag_analysis_node: {e}\", exc_info=True)\n",
    "        return {\"messages\": [AIMessage(content=\"I'm sorry, but I encountered a technical issue while processing your request. Please try again.\")]}\n",
    "\n",
    "# \n",
    "# LANGGRAPH-ENHANCED FINANCIAL SYSTEM (REFACTORED)\n",
    "# \n",
    "\n",
    "class LangGraphFinancialSystem:\n",
    "    \"\"\"Inference-focused system using pre-trained embeddings and GNN models.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SystemConfig):\n",
    "        self.config = config\n",
    "        logger.info(\" Initializing inference-ready Financial AI System...\")\n",
    "\n",
    "        self._initialize_ai_agents()\n",
    "        self._initialize_graph_components()\n",
    "        \n",
    "        self.workflow = self._build_workflow()\n",
    "        self.query_cache = LRUCache(maxsize=100)\n",
    "        \n",
    "        logger.info(\" System initialized successfully.\")\n",
    "        self.get_system_status()\n",
    "\n",
    "    def _initialize_ai_agents(self):\n",
    "        \"\"\"Initializes the core LLM and embedding models.\"\"\"\n",
    "        self.phi4_agent = FineTunedPhi4Agent(self.config)\n",
    "        self.e5_embedder = MultilingualE5Embedder(self.config.embedding_model)\n",
    "        self.numerical_agent = EnhancedNumericalReasoning(self.phi4_agent)\n",
    "        self.sec_api_tool = SECAPITool(user_agent=self.config.sec_user_agent)\n",
    "\n",
    "    def _initialize_graph_components(self):\n",
    "        \"\"\"Loads all graph-related assets: embeddings, GNN model, and RAG engine.\"\"\"\n",
    "        self.kg_agent = UnifiedKnowledgeGraphAgent(self.config, self.e5_embedder)\n",
    "        \n",
    "        self.gnn_model, self.kg_object = self._load_trained_gnn()\n",
    "        \n",
    "        self.graphrag_engine = self._build_graphrag_engine()\n",
    "        \n",
    "    def _load_trained_gnn(self) -> Tuple[Optional[nn.Module], Optional[Any]]:\n",
    "        \"\"\"Finds the latest training output and loads the GNN model and KG object.\"\"\"\n",
    "        try:\n",
    "            base_dir = \"./\"\n",
    "            training_dirs = sorted(\n",
    "                [d for d in os.listdir(base_dir) if d.startswith(\"trained_models_\") and os.path.isdir(d)],\n",
    "                reverse=True\n",
    "            )\n",
    "            if not training_dirs:\n",
    "                raise FileNotFoundError(\"No 'trained_models' directory found.\")\n",
    "\n",
    "            latest_dir = training_dirs[0]\n",
    "            model_path = os.path.join(latest_dir, \"attention_gnn.pth\")\n",
    "            kg_path = os.path.join(latest_dir, \"knowledge_graph.pkl\")\n",
    "\n",
    "            if not os.path.exists(model_path) or not os.path.exists(kg_path):\n",
    "                raise FileNotFoundError(f\"Model or KG not found in the latest directory: {latest_dir}\")\n",
    "\n",
    "            with open(kg_path, 'rb') as f:\n",
    "                kg = pickle.load(f)\n",
    "\n",
    "            input_dim = len(kg.feature_names)\n",
    "            \n",
    "            gnn_model = AttentionBasedFinancialGNN(input_dim=input_dim)\n",
    "            gnn_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            gnn_model.to(device)\n",
    "            gnn_model.eval()\n",
    "            \n",
    "            logger.info(f\" Loaded trained GNN and KG from '{latest_dir}'\")\n",
    "            return gnn_model, kg\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load trained GNN: {e}\", exc_info=True)\n",
    "            return None, None\n",
    "\n",
    "    def _build_graphrag_engine(self):\n",
    "        \"\"\"Builds the GraphRAG engine using the loaded knowledge graph.\"\"\"\n",
    "        if not self.kg_object:\n",
    "            logger.warning(\"Knowledge Graph object not loaded. Cannot build GraphRAG engine.\")\n",
    "            return None\n",
    "        \n",
    "        communities = self._build_communities(self.kg_object.graph)\n",
    "        if not communities:\n",
    "             logger.warning(\"Could not form communities. GraphRAG global search will be limited.\")\n",
    "             # Still return an engine, it can handle local queries without communities\n",
    "             \n",
    "        return FinancialGraphRAGQueryEngine(\n",
    "            kg_agent=self.kg_agent,\n",
    "            phi4_agent=self.phi4_agent,\n",
    "            communities=communities or {},\n",
    "            sec_api_tool=self.sec_api_tool\n",
    "        )\n",
    "    \n",
    "    def _build_communities(self, graph: nx.Graph) -> dict:\n",
    "        \"\"\"Builds and summarizes communities using the Louvain algorithm.\"\"\"\n",
    "        if graph.number_of_edges() == 0: \n",
    "            logger.warning(\"Graph has no edges. Cannot build communities.\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # Use Louvain algorithm which doesn't require flash attention\n",
    "            communities_list = louvain_communities(graph.to_undirected(), seed=42)\n",
    "            communities = {}\n",
    "            \n",
    "            for i, community_nodes in enumerate(communities_list):\n",
    "                if len(community_nodes) > 2:  # Only summarize meaningful communities\n",
    "                    entity_names = [graph.nodes[node].get('name', '') for node in community_nodes]\n",
    "                    entity_names = [name for name in entity_names if name]\n",
    "                    \n",
    "                    if entity_names:  # Only create summary if we have valid entity names\n",
    "                        prompt = f\"Summarize this group of companies in a single sentence under 20 words: {', '.join(entity_names[:10])}\"\n",
    "                        \n",
    "                        # FIX: Add try-catch around LLM generation to prevent community failures\n",
    "                        try:\n",
    "                            summary = self.phi4_agent.generate(prompt, max_new_tokens=50, temperature=0.1)\n",
    "                            if summary and summary.strip():  # Only add if summary is valid\n",
    "                                communities[i] = {'summary': summary, 'entities': list(community_nodes)}\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to generate summary for community {i}: {e}\")\n",
    "                            # Create a simple fallback summary\n",
    "                            communities[i] = {'summary': f\"Community of {len(entity_names)} companies\", 'entities': list(community_nodes)}\n",
    "            \n",
    "            logger.info(f\"Successfully created {len(communities)} communities\")\n",
    "            return communities\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Community detection failed: {e}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "    def _build_workflow(self):\n",
    "        \"\"\"Constructs the agentic workflow with a new branch for numerical reasoning.\"\"\"\n",
    "        workflow = StateGraph(FinancialState)\n",
    "\n",
    "        # 1. Define nodes for each step\n",
    "        analysis_node = partial(query_analysis_node, phi4_agent=self.phi4_agent)\n",
    "        gnn_node = partial(gnn_inference_node, gnn_model=self.gnn_model, kg=self.kg_object)\n",
    "        rag_node = partial(graphrag_analysis_node, graphrag_engine=self.graphrag_engine)\n",
    "        # ADD a new node for numerical reasoning\n",
    "        numerical_node = partial(numerical_reasoning_node, numerical_agent=self.numerical_agent)\n",
    "\n",
    "        # 2. Add all nodes to the graph\n",
    "        workflow.add_node(\"query_analysis\", analysis_node)\n",
    "        workflow.add_node(\"gnn_inference\", gnn_node)\n",
    "        workflow.add_node(\"generate_response\", rag_node)\n",
    "        workflow.add_node(\"numerical_reasoning\", numerical_node) # ADD the new node\n",
    "\n",
    "        # 3. Define the new 3-way routing logic\n",
    "        def route_query(state: FinancialState) -> Literal[\"local_path\", \"global_path\", \"numerical_path\"]:\n",
    "            query_type = state.get(\"analysis_results\", {}).get(\"query_type\")\n",
    "            if query_type == \"numerical\":\n",
    "                logger.info(\"Decision: Routing to numerical path.\")\n",
    "                return \"numerical_path\"\n",
    "            elif query_type == \"local\" and state.get(\"entities\"):\n",
    "                logger.info(\"Decision: Routing to local path (GNN).\")\n",
    "                return \"local_path\"\n",
    "            else:\n",
    "                logger.info(\"Decision: Routing to global path (RAG).\")\n",
    "                return \"global_path\"\n",
    "        \n",
    "        # 4. Construct the graph with the new conditional routing\n",
    "        workflow.set_entry_point(\"query_analysis\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"query_analysis\",\n",
    "            route_query,\n",
    "            {\n",
    "                \"local_path\": \"gnn_inference\",\n",
    "                \"global_path\": \"generate_response\",\n",
    "                \"numerical_path\": \"numerical_reasoning\" # ADD the new path\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 5. Define the end points for all branches\n",
    "        workflow.add_edge(\"gnn_inference\", \"generate_response\")\n",
    "        workflow.add_edge(\"generate_response\", END)\n",
    "        workflow.add_edge(\"numerical_reasoning\", END) # ADD the end for the new path\n",
    "        \n",
    "        return workflow.compile()\n",
    "\n",
    "\n",
    "    def stream_query(self, query: str):\n",
    "        \"\"\"\n",
    "        MODIFIED: Streams the workflow execution, yielding node-keyed updates.\n",
    "        \"\"\"\n",
    "        if not self.workflow:\n",
    "            yield {\"error\": \"Workflow not compiled.\"}\n",
    "            return\n",
    "            \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"kg_agent\": self.kg_agent\n",
    "        }\n",
    "        \n",
    "        # FIX: Use stream_mode=\"updates\" to get events like {\"node_name\": ...}\n",
    "        for event in self.workflow.stream(initial_state, stream_mode=\"updates\"):\n",
    "            yield event\n",
    "\n",
    "    def get_system_status(self):\n",
    "        status = {\n",
    "            \"available_embedding_models\": self.kg_agent.get_available_model_types(),\n",
    "            \"total_entities_loaded\": len(self.kg_agent.node_attributes),\n",
    "            \"gnn_model_loaded\": self.gnn_model is not None,\n",
    "            \"graphrag_engine_ready\": self.graphrag_engine is not None,\n",
    "        }\n",
    "        logger.info(f\" System Status: {status}\")\n",
    "        return status\n",
    "\n",
    "# \n",
    "# UTILITY FUNCTIONS\n",
    "# \n",
    "\n",
    "def search_companies_by_criteria(system: LangGraphFinancialSystem, criteria: List[str]):\n",
    "    \"\"\"Search companies by various criteria using the unified system\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Search by market cap\n",
    "    if 'large_cap' in criteria:\n",
    "        results['large_cap'] = system.kg_agent.storage.search_similar_entities(\n",
    "            query_text=\"large company\",\n",
    "            filters={\"$and\": [{\"market_cap\": {\"$gt\": 10e9}}, {\"type\": \"company\"}]},\n",
    "            top_k=10\n",
    "        )\n",
    "    \n",
    "    # Search by sector\n",
    "    if 'tech' in criteria:\n",
    "        results['tech'] = system.kg_agent.storage.search_similar_entities(\n",
    "            query_text=\"technology software computer\",\n",
    "            filters={\"type\": \"company\"},\n",
    "            top_k=10\n",
    "        )\n",
    "    \n",
    "    # Search by financial metrics\n",
    "    if 'high_pe' in criteria:\n",
    "        results['high_pe'] = system.kg_agent.storage.search_similar_entities(\n",
    "            query_text=\"growth stock\",\n",
    "            filters={\"$and\": [{\"pe_ratio\": {\"$gt\": 25}}, {\"type\": \"company\"}]},\n",
    "            top_k=10\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# \n",
    "# MAIN EXECUTION (INTERACTIVE) - FINAL CORRECTED VERSION\n",
    "# \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Initializing Financial AI System...\")\n",
    "    try:\n",
    "        config = SystemConfig()\n",
    "        system = LangGraphFinancialSystem(config)\n",
    "\n",
    "        if not system.workflow:\n",
    "            raise RuntimeError(\"System initialization failed. Workflow could not be compiled.\")\n",
    "\n",
    "        print(\"\\n System is ready. Enter your financial query or type 'exit' to quit.\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\n> \")\n",
    "            if query.lower() in ['exit', 'quit']:\n",
    "                print(\"Exiting system. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            print(\"Processing your query...\")\n",
    "            \n",
    "            \n",
    "            # Keep the latest state on every iteration to handle cases\n",
    "            # where the stream ends without an explicit \"__end__\" event.\n",
    "        \n",
    "            acc_state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"kg_agent\": system.kg_agent,\n",
    "            }\n",
    "            final_state = None\n",
    "\n",
    "            for event in system.stream_query(query):\n",
    "                if \"__end__\" in event:\n",
    "                    print(\"  -> Step completed: __end__\")\n",
    "                    # On the final event, the acc_state is the final_state\n",
    "                    final_state = acc_state\n",
    "                    break\n",
    "\n",
    "                node, payload = next(iter(event.items()))\n",
    "                print(f\"  -> Step completed: {node}\")\n",
    "                acc_state.update(payload)\n",
    "\n",
    "                # Continuously update final_state with the latest accumulated state\n",
    "                final_state = acc_state\n",
    "            \n",
    "            # This final assignment is a safeguard in case the loop finishes\n",
    "            # without ever assigning final_state inside the loop.\n",
    "            final_state = final_state or acc_state\n",
    "\n",
    "            print(\"\\n\" + \"=\"*50 + \" FINAL RESPONSE \" + \"=\"*50)\n",
    "            # Safely access messages from the guaranteed final_state\n",
    "            if final_state and final_state.get(\"messages\"):\n",
    "                print(final_state[\"messages\"][-1].content)\n",
    "            else:\n",
    "                print(\"ERROR: The workflow failed to produce a final message list.\")\n",
    "            print(\"=\"*116)\n",
    "            # =\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"A critical error occurred: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3370ab5a-55a4-42e5-8d24-dde02846adc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
